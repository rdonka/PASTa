{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"PASTa: Photometry Analysis and Signal Processing Toolbox Welcome to the documentation site for PASTa (Photometry Analysis and Signal Processing Toolbox)! The PASTa protocol is an open source toolbox and protocol for the preparation, signal processing, and analysis of fiber photometry data. Fiber photometry is a rapidly growing technique to record real-time neural signaling in awake, behaving subjects. However, the processing and analysis of photometry data streams can be complicated, and there is wide divergence in methods across the field. While several opensource signal processing tools exist, platforms can be inflexible in accommodating experimental designs, lack consistency in peak detection, and be difficult for naive users. The PASTa protocol is developed to be highly flexible and adaptable to a wide variety of experimental designs. While operating through MATLAB, the code is annotated to be readable, accessible, and adaptable for new users. Here you'll find set up instructions, a detailed user guide, example analyses, and additional details on function inputs and usage. For the toolbox and code repository, visit our GitHub .","title":"PASTa: Photometry Analysis and Signal Processing Toolbox"},{"location":"#pasta-photometry-analysis-and-signal-processing-toolbox","text":"Welcome to the documentation site for PASTa (Photometry Analysis and Signal Processing Toolbox)! The PASTa protocol is an open source toolbox and protocol for the preparation, signal processing, and analysis of fiber photometry data. Fiber photometry is a rapidly growing technique to record real-time neural signaling in awake, behaving subjects. However, the processing and analysis of photometry data streams can be complicated, and there is wide divergence in methods across the field. While several opensource signal processing tools exist, platforms can be inflexible in accommodating experimental designs, lack consistency in peak detection, and be difficult for naive users. The PASTa protocol is developed to be highly flexible and adaptable to a wide variety of experimental designs. While operating through MATLAB, the code is annotated to be readable, accessible, and adaptable for new users. Here you'll find set up instructions, a detailed user guide, example analyses, and additional details on function inputs and usage. For the toolbox and code repository, visit our GitHub .","title":"PASTa: Photometry Analysis and Signal Processing Toolbox"},{"location":"about/","text":"About PASTa was developed by the Roitman laboratories. For more information, or to request new features feel free to reach out! Dr. Jamie Roitman: https://jamiedroitman.wixsite.com/roitmandecisionlab Dr. Mitch Roitman: https://www.mroitmanlab.com/","title":"About"},{"location":"about/#about","text":"PASTa was developed by the Roitman laboratories. For more information, or to request new features feel free to reach out! Dr. Jamie Roitman: https://jamiedroitman.wixsite.com/roitmandecisionlab Dr. Mitch Roitman: https://www.mroitmanlab.com/","title":"About"},{"location":"exampleanalyses/","text":"Coming soon!","title":"Example Analyses"},{"location":"functiondocumentation/","text":"Function Documentation Overview This page contains additional documentation for each function within PASTa, as well as examples of inputs. Data Preparation Functions This set of functions is used to prepare raw photometry data, match it with experimental metadata, and load data into a structure in MATLAB. Functions are provided to handle data collected via TDT equipment and software Synapse, or a generic file structure with data streams saved to CSV files. loadKeys Combines subject key and file key into a data structure, and appends the provided computeruserpath to the paths in the file key. INPUTS: COMPUTERUSERPATH: A variable containing the unique portion of the file explorer path for the users specific computer. For example, 'C:\\Users\\rmdon\\'. Make sure the computeruserpath ends in a forward slash. SUBJECTKEYNAME: A variable containing a string with the name of the subject key csv file for the experiment (see \"3. Data Analysis Pipeline\" for more details). To omit a subject key and only load in the file key, set subjectkeyname = \"\". FILEKEYNAME: A variable containing a string with the name of the file key csv file for the experiment (see \"3. Data Analysis Pipeline\" for more details). OUTPUTS: EXPERIMENTKEY: A data structure called \"experimentkey\" that includes the joined file key and subject key with the computer user path appended to raw and extracted folder paths. EXAMPLE: computeruserpath = 'C:\\Users\\MYNAME\\'; % Computer specific portion of file navigation paths subjectkeyname = 'Subject Key.csv'; % Name of csv file containing subject information; set to '' if not using a subject key filekeyname = 'File Key.csv'; % Name of csv file containing session information, raw data folder names, and paths [experimentkey] = loadKeys(computeruserpath, subjectkeyname, filekeyname); % Load keys into a data structure called experimentkey NOTES: FILEKEY must contain at a minimum the fields Subject , RawFolderPath , and ExtractedFolderPath . SUBJECTKEY must contain at a minimum the field Subject Folder paths must end with a slash. The subject and file keys are joined based on Subject ID. Subject key must contain every subjects in the file key. If there is a mismatch, you will receive an error message that the right table does not contain all the key variables that are in the left table. The error message will display the unique subject IDs present in each key so you can determine where the mismatch occurred. Fields in subject and file key must be named uniquely. The only field that should be named the same in both keys is Subject. extractTDTdata This function is used to extract TDT data from saved blocks recorded via the software Synapse . For each block, extractTDTdata calls the function \"TDTbin2mat\" (TDT, 2019) and inputs the RawFolderPath to extract fiber photometry data recorded with Synapse. Extracted blocks are parsed it into a single data structure containing all fields, streams, and epocs. The function will identify the signal channel by matching the names in the input SIGSTREAMNAMES and the control channel by matching the names in the input BAQSTREAMNAMES. The name inputs can include a list of stream names if channel naming conventions vary by rig. Each block is saved as a separate data structure in a '.mat' file at the location specified by the inputs in extractedfolderpaths. INPUTS: RAWFOLDERPATHS: a string array containing the paths to the folder location of the raw data blocks to be extracted. The string array should contain one column with each full path in a separate row. EXTRACTEDFOLDERPATHS: a string array containing the paths to the folder location in which to save the extracted MatLab structs for each block to be extracted. The string array should contain one column with each full path in a separate row. SIGSTREAMNAMES: A cell array containing strings with the names of the streams to be treated as signal. Note that only one stream per file can be treated as signal. If different files have different stream names, include all stream names in the cell array. BAQSTREAMNAMES: A cell array containing strings with the names of the streams to be treated as background. Note that only one stream per file can be treated as background. If different files have different stream names, include all stream names in the cell array. OPTIONAL INPUTS: CLIP: the number of seconds to remove on either end of the data streams. If not specified, defaults to 5 seconds. SKIPEXISTING: A binary variable containing a 0 if pre-existing extracted blocks should be re-extracted or a 1 if pre-existing extracted blocks should be skipped. This allows the user to toggle whether or not to extract every block, or only blocks that have not previously been extracted. If not specified, defaults to 1 (skip previously extracted blocks). OUTPUTS: Saved .mat data structures for each block in the location specified by extractedfolderpaths. EXAMPLE - DEFAULT: sigstreamnames = {'x65A', '465A'}; % All names of signal streams across files baqstreamnames = {'x05A', '405A'}; % All names of background streams across files rawfolderpaths = string({experimentkey.RawFolderPath})'; % Create string array of raw folder paths extractedfolderpaths = string({experimentkey.ExtractedFolderPath})'; % Create string array of extracted folder paths extractTDTdata(rawfolderpaths,extractedfolderpaths,sigstreamnames,baqstreamnames,clip,skipexisting); % extract data EXAMPLE - MANUALLY SPECIFIED CLIP AND SKIPEXISTING: clip = 3; skipexisting = 0; sigstreamnames = {'x65A', '465A'}; % All names of signal streams across files baqstreamnames = {'x05A', '405A'}; % All names of background streams across files rawfolderpaths = string({experimentkey.RawFolderPath})'; % Create string array of raw folder paths' extractedfolderpaths = string({experimentkey.ExtractedFolderPath})'; % Create string array of extracted folder paths' extractTDTdata(rawfolderpaths,extractedfolderpaths,sigstreamnames,baqstreamnames,'clip',clip,'skipexisting',skipexisting); % extract data loadTDTdata For use with previously extracted data collect with TDT equipment and software Synapse. loadTDTData loads previously extracted .mat data blocks into a data structure for further analysis. Each block is one row. The input experiment key must include at a minimum the extracted folder path location of the block. Any additional information about the subject and session in the experiment key will be matched to the extracted data. INPUTS: EXPERIMENTKEY: A prepared data structure with at minimum the ExtractedFolderPath to locate the individual block structures to be loaded. OUTPUTS: DATA: the input data structure with each individual extracted block added by row. EXAMPLE: [rawdata] = loadKeydata(experimentkey); % Load data based on the experiment key into the structure 'rawdata' loadCSVdata For use with data collected and stored to a general file structure. coming soon trimFPdata Used to trim samples at the very start and end of recordings that are not to be included in analysis (such as the the first two minutes of the session, or the first samples before a hardware control program is initiated). Trims all specified data streams from the index in trimstart to the index in trimend, and adjusts epocs by the amount trimmed by trimstart. Users must pre-prepare the trim start and end indexes to specify as inputs for the function. INPUTS: DATA: A data frame containing at least the specified input fields. TRIMSTART: The location to start trimming at. TRIMEND: The location to end trimming at. WHICHSTREAMS: A cell array containing the names of all the streams to be trimmed. OPTIONAL INPUTS: WHICHEPOCS: A cell array containing the names of all the epocs to be adjusted due to trimming - subtract the (start loc - 1) from the epoc. OUTPUTS: DATA: The data structure with the specified data stream containing the trimmed data. EXAMPLE: trimstart = 'sessionstart'; % name of field with session start index trimend = 'sessionend'; % name of field with session end index whichstreams = {'sig', 'baq','time'}; % which streams to trim whichepocs = {'injt','sess'}; % which epocs to adjust to maintain relative position [data] = trimFPdata(rawdata,trimstart,trimend, whichstreams,whichepocs); % Output trimmed data into new structure called data Signal Processing Functions subtractFPdata Used to subtract the background photometry stream (eg, 405nm) from the signal stream (eg, 465nm), convert the subtracted signal to delta F/f, and apply a filter to denoise the output. Users must input the data structure with the raw data, the names of the fields containing the signal and background streams, and the sampling rate of the collected data. INPUTS: DATA: A data frame containing at least the specified input fields. SIGFIELD: The name (string) of the field containing the signal stream. BAQFIELD: The name (string) of the field containing the background stream. FS: The sampling rate of the raw data collection in hz. OPTIONAL INPUTS: BAQSCALINGTYPE: A string to specify the type of background scaling to apply. Options are 'frequency', 'sigmean', 'OLS', 'detrendOLS', 'smoothedOLS', or 'IRLS'. Default: 'frequency'. 'frequency': Scales the background to the signal channel based on ratio of specified frequency bands in the FFT (frequency domain) of the channels. 'sigmean': Scales the background to the signal channel based on the ratio of the mean of the signal to the mean of the background (time domain). 'OLS': Uses ordinary least-squares regression to generate scaled background. 'detrendOLS': Removes the linear trend from signal and background streams prior to using ordinary least-squares regression to generate scaled background. 'smoothedOLS': Applies lowess smoothing to the signal and background streams prior to using ordinary least-squares regression to generate scaled background. 'IRLS': Uses iteratively reweighted least squares regression to generate scaled background. BAQSCALINGFREQ: Only used with 'frequency' scaling. Numeric frequency (Hz) threshold for scaling the background to signal channel. Frequencies above this value will be included in the scaling factor determination. Default: 10 Hz. BAQSCALINGPERC: Only used with 'frequency' and 'sigmean' scaling. Adjusts the background scaling factor to be a percednt of the derived scaling factor value. Default: 1 (100%). SUBTRACTIONOUTPUT: Output type for the subtracted data. Default: 'dff' 'dff': Outputs subtracted signal as delta F/F. 'df': Outputs subtracted signal as delta F. FILTERTYPE: A string to specify the type of filter to apply after subtraction. Default: 'bandpass'. 'nofilter': No filter will be applied. 'bandpass': A bandpass filter will be applied. 'highpass': Only the high pass filter will be applied. 'lowpass': Only the low pass filter will be applied. PADDING: Defaults to 1, which applies padding. Padding takes the first 10% of the stream, flips it, and appends it to the data before filtering. Appended data is trimmed after filtration. Set to 0 to turn off padding of data streams. Default: 1. PADDINGPERC: Percent of data length to use to determine the number of samples to be appended to the beginning and end of data in padding. Set to minimum 10%. Default: 0.1 (10%). FILTERORDER: The order to be used for the chosen butterworth filter. Default: 3. HIGHPASSCUTOFF: The cutoff frequency (hz) to be used for the high pass butterworth filter. Default: 2.2860. LOWPASSCUTOFF: The cutoff to be used for the low pass butterworth filter. Default: 0.0051. NOTE: 'bandpass' applies both the high and low cutoffs to design the filter. SUPRESSDISP: If set to anything other than 0, this will suppress the command window displays. Default: 0. OUTPUTS: DATA: The original data structure with added fields with the scaled background ('baq_scaled'), subtracted signal ('sigsub'), and subtracted and filtered signal ('sigfilt'). All inputs and defaults will be added to the data structure under the field 'inputs'. NOTE: If using BAQSCALINGMETHOD 'detrendOLS', additional fields containing the detrended signal and background ('sig_detrend' and 'baq_detrend') will be added to the data frame. If using BAQSCALINGMETHOD 'smoothedOLS', additional fields containing the smoothed signal and background ('sig_smoothed' and 'baq_smoothed') will be added to the data frame. EXAMPLE - DEFAULT: sigfield = 'sig'; baqfield = 'baq'; fs = 1017; data = subtractFPdata(data,sigfield,baqfield,fs); EXAMPLE - Frequency Scaling with 20Hz Threshold and Highpass Filter Only: sigfield = 'sig'; baqfield = 'baq'; fs = 1017; data = subtractFPdata(data,sigfield,baqfield,fs,'baqscalingfreq',20,'filtertype,'highpass'); Normalization Peak Detection and Quantification Functions findpeaks peakarea Individual Trial Analyses Functions cutTrialData trialAverage trialnormalization Plotting Functions Whole Session Plots FFT Plots Peak Plots Trial Plots","title":"Function Documentation"},{"location":"functiondocumentation/#function-documentation-overview","text":"This page contains additional documentation for each function within PASTa, as well as examples of inputs.","title":"Function Documentation Overview"},{"location":"functiondocumentation/#data-preparation-functions","text":"This set of functions is used to prepare raw photometry data, match it with experimental metadata, and load data into a structure in MATLAB. Functions are provided to handle data collected via TDT equipment and software Synapse, or a generic file structure with data streams saved to CSV files.","title":"Data Preparation Functions"},{"location":"functiondocumentation/#loadkeys","text":"Combines subject key and file key into a data structure, and appends the provided computeruserpath to the paths in the file key. INPUTS: COMPUTERUSERPATH: A variable containing the unique portion of the file explorer path for the users specific computer. For example, 'C:\\Users\\rmdon\\'. Make sure the computeruserpath ends in a forward slash. SUBJECTKEYNAME: A variable containing a string with the name of the subject key csv file for the experiment (see \"3. Data Analysis Pipeline\" for more details). To omit a subject key and only load in the file key, set subjectkeyname = \"\". FILEKEYNAME: A variable containing a string with the name of the file key csv file for the experiment (see \"3. Data Analysis Pipeline\" for more details). OUTPUTS: EXPERIMENTKEY: A data structure called \"experimentkey\" that includes the joined file key and subject key with the computer user path appended to raw and extracted folder paths. EXAMPLE: computeruserpath = 'C:\\Users\\MYNAME\\'; % Computer specific portion of file navigation paths subjectkeyname = 'Subject Key.csv'; % Name of csv file containing subject information; set to '' if not using a subject key filekeyname = 'File Key.csv'; % Name of csv file containing session information, raw data folder names, and paths [experimentkey] = loadKeys(computeruserpath, subjectkeyname, filekeyname); % Load keys into a data structure called experimentkey NOTES: FILEKEY must contain at a minimum the fields Subject , RawFolderPath , and ExtractedFolderPath . SUBJECTKEY must contain at a minimum the field Subject Folder paths must end with a slash. The subject and file keys are joined based on Subject ID. Subject key must contain every subjects in the file key. If there is a mismatch, you will receive an error message that the right table does not contain all the key variables that are in the left table. The error message will display the unique subject IDs present in each key so you can determine where the mismatch occurred. Fields in subject and file key must be named uniquely. The only field that should be named the same in both keys is Subject.","title":"loadKeys"},{"location":"functiondocumentation/#extracttdtdata","text":"This function is used to extract TDT data from saved blocks recorded via the software Synapse . For each block, extractTDTdata calls the function \"TDTbin2mat\" (TDT, 2019) and inputs the RawFolderPath to extract fiber photometry data recorded with Synapse. Extracted blocks are parsed it into a single data structure containing all fields, streams, and epocs. The function will identify the signal channel by matching the names in the input SIGSTREAMNAMES and the control channel by matching the names in the input BAQSTREAMNAMES. The name inputs can include a list of stream names if channel naming conventions vary by rig. Each block is saved as a separate data structure in a '.mat' file at the location specified by the inputs in extractedfolderpaths. INPUTS: RAWFOLDERPATHS: a string array containing the paths to the folder location of the raw data blocks to be extracted. The string array should contain one column with each full path in a separate row. EXTRACTEDFOLDERPATHS: a string array containing the paths to the folder location in which to save the extracted MatLab structs for each block to be extracted. The string array should contain one column with each full path in a separate row. SIGSTREAMNAMES: A cell array containing strings with the names of the streams to be treated as signal. Note that only one stream per file can be treated as signal. If different files have different stream names, include all stream names in the cell array. BAQSTREAMNAMES: A cell array containing strings with the names of the streams to be treated as background. Note that only one stream per file can be treated as background. If different files have different stream names, include all stream names in the cell array. OPTIONAL INPUTS: CLIP: the number of seconds to remove on either end of the data streams. If not specified, defaults to 5 seconds. SKIPEXISTING: A binary variable containing a 0 if pre-existing extracted blocks should be re-extracted or a 1 if pre-existing extracted blocks should be skipped. This allows the user to toggle whether or not to extract every block, or only blocks that have not previously been extracted. If not specified, defaults to 1 (skip previously extracted blocks). OUTPUTS: Saved .mat data structures for each block in the location specified by extractedfolderpaths. EXAMPLE - DEFAULT: sigstreamnames = {'x65A', '465A'}; % All names of signal streams across files baqstreamnames = {'x05A', '405A'}; % All names of background streams across files rawfolderpaths = string({experimentkey.RawFolderPath})'; % Create string array of raw folder paths extractedfolderpaths = string({experimentkey.ExtractedFolderPath})'; % Create string array of extracted folder paths extractTDTdata(rawfolderpaths,extractedfolderpaths,sigstreamnames,baqstreamnames,clip,skipexisting); % extract data EXAMPLE - MANUALLY SPECIFIED CLIP AND SKIPEXISTING: clip = 3; skipexisting = 0; sigstreamnames = {'x65A', '465A'}; % All names of signal streams across files baqstreamnames = {'x05A', '405A'}; % All names of background streams across files rawfolderpaths = string({experimentkey.RawFolderPath})'; % Create string array of raw folder paths' extractedfolderpaths = string({experimentkey.ExtractedFolderPath})'; % Create string array of extracted folder paths' extractTDTdata(rawfolderpaths,extractedfolderpaths,sigstreamnames,baqstreamnames,'clip',clip,'skipexisting',skipexisting); % extract data","title":"extractTDTdata"},{"location":"functiondocumentation/#loadtdtdata","text":"For use with previously extracted data collect with TDT equipment and software Synapse. loadTDTData loads previously extracted .mat data blocks into a data structure for further analysis. Each block is one row. The input experiment key must include at a minimum the extracted folder path location of the block. Any additional information about the subject and session in the experiment key will be matched to the extracted data. INPUTS: EXPERIMENTKEY: A prepared data structure with at minimum the ExtractedFolderPath to locate the individual block structures to be loaded. OUTPUTS: DATA: the input data structure with each individual extracted block added by row. EXAMPLE: [rawdata] = loadKeydata(experimentkey); % Load data based on the experiment key into the structure 'rawdata'","title":"loadTDTdata"},{"location":"functiondocumentation/#loadcsvdata","text":"For use with data collected and stored to a general file structure. coming soon","title":"loadCSVdata"},{"location":"functiondocumentation/#trimfpdata","text":"Used to trim samples at the very start and end of recordings that are not to be included in analysis (such as the the first two minutes of the session, or the first samples before a hardware control program is initiated). Trims all specified data streams from the index in trimstart to the index in trimend, and adjusts epocs by the amount trimmed by trimstart. Users must pre-prepare the trim start and end indexes to specify as inputs for the function. INPUTS: DATA: A data frame containing at least the specified input fields. TRIMSTART: The location to start trimming at. TRIMEND: The location to end trimming at. WHICHSTREAMS: A cell array containing the names of all the streams to be trimmed. OPTIONAL INPUTS: WHICHEPOCS: A cell array containing the names of all the epocs to be adjusted due to trimming - subtract the (start loc - 1) from the epoc. OUTPUTS: DATA: The data structure with the specified data stream containing the trimmed data. EXAMPLE: trimstart = 'sessionstart'; % name of field with session start index trimend = 'sessionend'; % name of field with session end index whichstreams = {'sig', 'baq','time'}; % which streams to trim whichepocs = {'injt','sess'}; % which epocs to adjust to maintain relative position [data] = trimFPdata(rawdata,trimstart,trimend, whichstreams,whichepocs); % Output trimmed data into new structure called data","title":"trimFPdata"},{"location":"functiondocumentation/#signal-processing-functions","text":"","title":"Signal Processing Functions"},{"location":"functiondocumentation/#subtractfpdata","text":"Used to subtract the background photometry stream (eg, 405nm) from the signal stream (eg, 465nm), convert the subtracted signal to delta F/f, and apply a filter to denoise the output. Users must input the data structure with the raw data, the names of the fields containing the signal and background streams, and the sampling rate of the collected data. INPUTS: DATA: A data frame containing at least the specified input fields. SIGFIELD: The name (string) of the field containing the signal stream. BAQFIELD: The name (string) of the field containing the background stream. FS: The sampling rate of the raw data collection in hz. OPTIONAL INPUTS: BAQSCALINGTYPE: A string to specify the type of background scaling to apply. Options are 'frequency', 'sigmean', 'OLS', 'detrendOLS', 'smoothedOLS', or 'IRLS'. Default: 'frequency'. 'frequency': Scales the background to the signal channel based on ratio of specified frequency bands in the FFT (frequency domain) of the channels. 'sigmean': Scales the background to the signal channel based on the ratio of the mean of the signal to the mean of the background (time domain). 'OLS': Uses ordinary least-squares regression to generate scaled background. 'detrendOLS': Removes the linear trend from signal and background streams prior to using ordinary least-squares regression to generate scaled background. 'smoothedOLS': Applies lowess smoothing to the signal and background streams prior to using ordinary least-squares regression to generate scaled background. 'IRLS': Uses iteratively reweighted least squares regression to generate scaled background. BAQSCALINGFREQ: Only used with 'frequency' scaling. Numeric frequency (Hz) threshold for scaling the background to signal channel. Frequencies above this value will be included in the scaling factor determination. Default: 10 Hz. BAQSCALINGPERC: Only used with 'frequency' and 'sigmean' scaling. Adjusts the background scaling factor to be a percednt of the derived scaling factor value. Default: 1 (100%). SUBTRACTIONOUTPUT: Output type for the subtracted data. Default: 'dff' 'dff': Outputs subtracted signal as delta F/F. 'df': Outputs subtracted signal as delta F. FILTERTYPE: A string to specify the type of filter to apply after subtraction. Default: 'bandpass'. 'nofilter': No filter will be applied. 'bandpass': A bandpass filter will be applied. 'highpass': Only the high pass filter will be applied. 'lowpass': Only the low pass filter will be applied. PADDING: Defaults to 1, which applies padding. Padding takes the first 10% of the stream, flips it, and appends it to the data before filtering. Appended data is trimmed after filtration. Set to 0 to turn off padding of data streams. Default: 1. PADDINGPERC: Percent of data length to use to determine the number of samples to be appended to the beginning and end of data in padding. Set to minimum 10%. Default: 0.1 (10%). FILTERORDER: The order to be used for the chosen butterworth filter. Default: 3. HIGHPASSCUTOFF: The cutoff frequency (hz) to be used for the high pass butterworth filter. Default: 2.2860. LOWPASSCUTOFF: The cutoff to be used for the low pass butterworth filter. Default: 0.0051. NOTE: 'bandpass' applies both the high and low cutoffs to design the filter. SUPRESSDISP: If set to anything other than 0, this will suppress the command window displays. Default: 0. OUTPUTS: DATA: The original data structure with added fields with the scaled background ('baq_scaled'), subtracted signal ('sigsub'), and subtracted and filtered signal ('sigfilt'). All inputs and defaults will be added to the data structure under the field 'inputs'. NOTE: If using BAQSCALINGMETHOD 'detrendOLS', additional fields containing the detrended signal and background ('sig_detrend' and 'baq_detrend') will be added to the data frame. If using BAQSCALINGMETHOD 'smoothedOLS', additional fields containing the smoothed signal and background ('sig_smoothed' and 'baq_smoothed') will be added to the data frame. EXAMPLE - DEFAULT: sigfield = 'sig'; baqfield = 'baq'; fs = 1017; data = subtractFPdata(data,sigfield,baqfield,fs); EXAMPLE - Frequency Scaling with 20Hz Threshold and Highpass Filter Only: sigfield = 'sig'; baqfield = 'baq'; fs = 1017; data = subtractFPdata(data,sigfield,baqfield,fs,'baqscalingfreq',20,'filtertype,'highpass');","title":"subtractFPdata"},{"location":"functiondocumentation/#normalization","text":"","title":"Normalization"},{"location":"functiondocumentation/#peak-detection-and-quantification-functions","text":"","title":"Peak Detection and Quantification Functions"},{"location":"functiondocumentation/#findpeaks","text":"","title":"findpeaks"},{"location":"functiondocumentation/#peakarea","text":"","title":"peakarea"},{"location":"functiondocumentation/#individual-trial-analyses-functions","text":"","title":"Individual Trial Analyses Functions"},{"location":"functiondocumentation/#cuttrialdata","text":"","title":"cutTrialData"},{"location":"functiondocumentation/#trialaverage","text":"","title":"trialAverage"},{"location":"functiondocumentation/#trialnormalization","text":"","title":"trialnormalization"},{"location":"functiondocumentation/#plotting-functions","text":"","title":"Plotting Functions"},{"location":"functiondocumentation/#whole-session-plots","text":"","title":"Whole Session Plots"},{"location":"functiondocumentation/#fft-plots","text":"","title":"FFT Plots"},{"location":"functiondocumentation/#peak-plots","text":"","title":"Peak Plots"},{"location":"functiondocumentation/#trial-plots","text":"","title":"Trial Plots"},{"location":"gettingstarted/","text":"Welcome to the PASTa Quick Start Guide! PASTa (Photometry Analysis and Signal Processing Toolbox) is a MATLAB toolbox developed for signal processing anad analysis of fiber photometry data. The toolbox is written to be user-friendly and approachable for people at all levels of coding, with flexible functions to make analysis of a wide variety of experimental designs feasible. Functions are included for data management and organization, signal processing, and experimental analyses including whole session and event/trial specific designs. Here you'll find the quick start guide to PASTa with instructions on set up and installation, and instructions for basic use of the toolbox. For full documentation visit mkdocs.org . Set Up and Installation GitHub and GitHub Desktop Install GitHub Desktop if you haven't already. Make an account on GitHub, note that students get access to premium features for free through the student developer pack once verified. Install GitHub Desktop and set up a location for your repositories on your local computer. For newer users, we recommend making a folder on your desktop called \"GitHubRepositories\" and cloning all repositories there. This makes it very easy to locate your repositories. For an overview of how to use GitHub and best practices: https://www.freecodecamp.org/news/introduction-to-git-and-github/ MATLAB Install MATLAB to you computer, including the simulink suite. Functions have been tested with MALTAB 2022, 2023, and 2024 releases.","title":"Getting Started"},{"location":"gettingstarted/#welcome-to-the-pasta-quick-start-guide","text":"PASTa (Photometry Analysis and Signal Processing Toolbox) is a MATLAB toolbox developed for signal processing anad analysis of fiber photometry data. The toolbox is written to be user-friendly and approachable for people at all levels of coding, with flexible functions to make analysis of a wide variety of experimental designs feasible. Functions are included for data management and organization, signal processing, and experimental analyses including whole session and event/trial specific designs. Here you'll find the quick start guide to PASTa with instructions on set up and installation, and instructions for basic use of the toolbox. For full documentation visit mkdocs.org .","title":"Welcome to the PASTa Quick Start Guide!"},{"location":"gettingstarted/#set-up-and-installation","text":"","title":"Set Up and Installation"},{"location":"gettingstarted/#github-and-github-desktop","text":"Install GitHub Desktop if you haven't already. Make an account on GitHub, note that students get access to premium features for free through the student developer pack once verified. Install GitHub Desktop and set up a location for your repositories on your local computer. For newer users, we recommend making a folder on your desktop called \"GitHubRepositories\" and cloning all repositories there. This makes it very easy to locate your repositories. For an overview of how to use GitHub and best practices: https://www.freecodecamp.org/news/introduction-to-git-and-github/","title":"GitHub and GitHub Desktop"},{"location":"gettingstarted/#matlab","text":"Install MATLAB to you computer, including the simulink suite. Functions have been tested with MALTAB 2022, 2023, and 2024 releases.","title":"MATLAB"},{"location":"userguide/datapreparation/","text":"Data Preparation The first stage in the PASTa Protocol is data preparation, which involves associating fiber photometry data with experimental meta data, loading raw data, and extracting and saving the raw data as MATLAB data structures to faciliate future analysis sessions. For data collected with Tucker Davis Technologies equipment and software Synapse , custom functions are included to extract data from raw formats. For data collected with other systems, data must be pre-formatted to match a generic csv file format structure and loaded with provided custom functions, or prepared separately by the user. If your data doesn't match the available load options, please feel free to reach out! Data Organization The PASTa protocol is set up to accomodate any kind of file organization preferred by the user. Organization preference may vary depending on your lab's file storage practices, photometry equipment, or individual projects. TDT Synapse Output Fiber photometry data collected through the software Synapse (Tucker Davis Technologies) is stored in tanks and blocks . Tanks are parent folders created by Synapse for each experiment. Blocks are individual folders for each session containing the actual output data. Stored data cannot be accessed directly via the folder, but rather must be extracted via MATLAB. By default, the tank path is: C:\\TDT\\Synapse\\Tanks. Synapse recognizes experiments and subjects as key categories of information that play a special role in managing data storage and retrieval. Thus, when running the session, it is critical to ensure the correct Experiment profile is selected, and the correct Subject identifier is input for each session. By default, Synapse names data tanks automatically based on experiment name and the start time of the first recording {ExperimentName}-{yymmdd}-{hhmmss}. Blocks of data are named based on subject {SubjectName}-{yymmdd}-{hhmmss} for each recording session and the start time. Synapse will save a new Tank for every day unless you change the default setting. Click Menu at the top of the bar, then Preferences. Under the Data Saving tab, make sure \"New Tank Each Day\" is unchecked. Experiment Key Creation To accomodate a variety of file organization structures, users can first create two csv files containing the information necessary to access raw data, and experimental metadata to match to raw photometry data. MATLAB can access raw data folders stored either locally or in a cloud-based storage app like Box or Dropbox. To faciliate analysis, users can create two keys as csv files: a subject key and a file key. In the PASTa protocol, these files will be knit together to pair subject specific information with each individual session of data, preventing the need for manual repeated entry of subject specific information and reduce the time burden of properly maintaining and including experimental metadata factors like subject traits, treatments, and experimental equipment. First, locate the raw files to be analyzed in your file organization structure. Prepare a folder for extracted raw data to be saved to. This should be separate from your tank folder / raw data storage, and will eventually contain the extracted data sets for each session as MATLAB structures to facilitate efficient data processing in future analysis sessions. Subject Key The subject key should contain information about each subject that is constant and unchanging, such as SubjectID, sex, date of birth, fiber location, sensor, experimental group, and any other user specificed information. For example: File Key The file key should contain information about each unique session / file to be analyzed. At a minimum, this must include the SubjectID, folder name, raw folder location, and desired location for the raw data to be exported to. SubjectID: Unique identifier of the subject. This fieldname should match the first field in the subject key, and inputs for each subject have to match the subject key for subject specific data to be properly associated to fiber photometry data. Folder: The name of the folder containing the raw data. RawFolderPath : The path to the location where the raw data folder is saved for each specific session. All file paths should be specified in the file key WITHOUT the computer user specific portion of the path. This facilitates analysis across multiple devices or cloud storage solutions without manual edits to the file key. Ensure the specified path ends in a forward slash. For example , a file saved to a specific device at the path \"C:\\Users\\rmdon\\Box\\RawData\\\" should be specified as \"Box\\RawData\\\" . ExtractedFolderPath: The path to the location where you would like the extracted data structure to be saved. This should be different than the raw data storage location. All file paths should be specified in the file key WITHOUT the computer and user specific portion of the path. This facilitates analysis across multiple devices or cloud storage solutions without manual edits to the file key. Ensure the specified path ends in a forward slash. For example , a file saved to a specific device at the path \"C:\\Users\\rmdon\\Box\\ExtractedData\\\" should be specified as \"Box\\ExtractedData\\\" . Any additional fields can be included such as equipment information, recording power, session condition, drug treatments, body weight, and any other variables that are specific to that one session. Note that the only field name that should overlap with a field name in the subject key is SubjectID . For example: Making the Experiment Key The function loadKeys joins the individual subject information to the file key with the data for each session. Additionally, loadkeys appends the unique computer user portion of the file navigation path to the beginning and the Folder name to the end of the raw and extracted folder paths specified in the file key. This creates the full path to the location of each session's data. For example, \"C:\\Users\\rmdon\\Box\\RawData\\Subject1-240101-121500\" . The created experiment key should be output into a data structure called experimentkey . Code example: Extracting the Data Prior to beginning analysis, individual session data should be extracted and saved as MATLAB data structures. This makes the process of loading data at the start of each analysis session significantly faster. Two options are available to extract the data, and should be used depending on the method by which the data was collected, both documented in detail below. Data collected with TDT can be extracted with custom functions. For all other systems, utilize the generic csv format: for d If data were collected with TDT's Synapse, use the function loadTDTdata . If data were collected with other systems, use the generic format and load function loadCSVdata . To load TDT data, use the function __extract Set up inputs \"C:\\Users\\rmdon\\Box\\RawData\\Subject1-240101-121500\"","title":"Data Preparation"},{"location":"userguide/datapreparation/#data-preparation","text":"The first stage in the PASTa Protocol is data preparation, which involves associating fiber photometry data with experimental meta data, loading raw data, and extracting and saving the raw data as MATLAB data structures to faciliate future analysis sessions. For data collected with Tucker Davis Technologies equipment and software Synapse , custom functions are included to extract data from raw formats. For data collected with other systems, data must be pre-formatted to match a generic csv file format structure and loaded with provided custom functions, or prepared separately by the user. If your data doesn't match the available load options, please feel free to reach out!","title":"Data Preparation"},{"location":"userguide/datapreparation/#data-organization","text":"The PASTa protocol is set up to accomodate any kind of file organization preferred by the user. Organization preference may vary depending on your lab's file storage practices, photometry equipment, or individual projects.","title":"Data Organization"},{"location":"userguide/datapreparation/#tdt-synapse-output","text":"Fiber photometry data collected through the software Synapse (Tucker Davis Technologies) is stored in tanks and blocks . Tanks are parent folders created by Synapse for each experiment. Blocks are individual folders for each session containing the actual output data. Stored data cannot be accessed directly via the folder, but rather must be extracted via MATLAB. By default, the tank path is: C:\\TDT\\Synapse\\Tanks. Synapse recognizes experiments and subjects as key categories of information that play a special role in managing data storage and retrieval. Thus, when running the session, it is critical to ensure the correct Experiment profile is selected, and the correct Subject identifier is input for each session. By default, Synapse names data tanks automatically based on experiment name and the start time of the first recording {ExperimentName}-{yymmdd}-{hhmmss}. Blocks of data are named based on subject {SubjectName}-{yymmdd}-{hhmmss} for each recording session and the start time. Synapse will save a new Tank for every day unless you change the default setting. Click Menu at the top of the bar, then Preferences. Under the Data Saving tab, make sure \"New Tank Each Day\" is unchecked.","title":"TDT Synapse Output"},{"location":"userguide/datapreparation/#experiment-key-creation","text":"To accomodate a variety of file organization structures, users can first create two csv files containing the information necessary to access raw data, and experimental metadata to match to raw photometry data. MATLAB can access raw data folders stored either locally or in a cloud-based storage app like Box or Dropbox. To faciliate analysis, users can create two keys as csv files: a subject key and a file key. In the PASTa protocol, these files will be knit together to pair subject specific information with each individual session of data, preventing the need for manual repeated entry of subject specific information and reduce the time burden of properly maintaining and including experimental metadata factors like subject traits, treatments, and experimental equipment. First, locate the raw files to be analyzed in your file organization structure. Prepare a folder for extracted raw data to be saved to. This should be separate from your tank folder / raw data storage, and will eventually contain the extracted data sets for each session as MATLAB structures to facilitate efficient data processing in future analysis sessions.","title":"Experiment Key Creation"},{"location":"userguide/datapreparation/#subject-key","text":"The subject key should contain information about each subject that is constant and unchanging, such as SubjectID, sex, date of birth, fiber location, sensor, experimental group, and any other user specificed information. For example:","title":"Subject Key"},{"location":"userguide/datapreparation/#file-key","text":"The file key should contain information about each unique session / file to be analyzed. At a minimum, this must include the SubjectID, folder name, raw folder location, and desired location for the raw data to be exported to. SubjectID: Unique identifier of the subject. This fieldname should match the first field in the subject key, and inputs for each subject have to match the subject key for subject specific data to be properly associated to fiber photometry data. Folder: The name of the folder containing the raw data. RawFolderPath : The path to the location where the raw data folder is saved for each specific session. All file paths should be specified in the file key WITHOUT the computer user specific portion of the path. This facilitates analysis across multiple devices or cloud storage solutions without manual edits to the file key. Ensure the specified path ends in a forward slash. For example , a file saved to a specific device at the path \"C:\\Users\\rmdon\\Box\\RawData\\\" should be specified as \"Box\\RawData\\\" . ExtractedFolderPath: The path to the location where you would like the extracted data structure to be saved. This should be different than the raw data storage location. All file paths should be specified in the file key WITHOUT the computer and user specific portion of the path. This facilitates analysis across multiple devices or cloud storage solutions without manual edits to the file key. Ensure the specified path ends in a forward slash. For example , a file saved to a specific device at the path \"C:\\Users\\rmdon\\Box\\ExtractedData\\\" should be specified as \"Box\\ExtractedData\\\" . Any additional fields can be included such as equipment information, recording power, session condition, drug treatments, body weight, and any other variables that are specific to that one session. Note that the only field name that should overlap with a field name in the subject key is SubjectID . For example:","title":"File Key"},{"location":"userguide/datapreparation/#making-the-experiment-key","text":"The function loadKeys joins the individual subject information to the file key with the data for each session. Additionally, loadkeys appends the unique computer user portion of the file navigation path to the beginning and the Folder name to the end of the raw and extracted folder paths specified in the file key. This creates the full path to the location of each session's data. For example, \"C:\\Users\\rmdon\\Box\\RawData\\Subject1-240101-121500\" . The created experiment key should be output into a data structure called experimentkey . Code example:","title":"Making the Experiment Key"},{"location":"userguide/datapreparation/#extracting-the-data","text":"Prior to beginning analysis, individual session data should be extracted and saved as MATLAB data structures. This makes the process of loading data at the start of each analysis session significantly faster. Two options are available to extract the data, and should be used depending on the method by which the data was collected, both documented in detail below. Data collected with TDT can be extracted with custom functions. For all other systems, utilize the generic csv format: for d If data were collected with TDT's Synapse, use the function loadTDTdata . If data were collected with other systems, use the generic format and load function loadCSVdata . To load TDT data, use the function __extract","title":"Extracting the Data"},{"location":"userguide/datapreparation/#set-up-inputs","text":"\"C:\\Users\\rmdon\\Box\\RawData\\Subject1-240101-121500\"","title":"Set up inputs"},{"location":"userguide/userguide/","text":"PASTa User Guide This user guide is meant to serve as a detailed step-by-step practical guide through preparing, processing, and analyzing fiber photometry data with the PASTa protocol.","title":"User Guide"},{"location":"userguide/userguide/#pasta-user-guide","text":"This user guide is meant to serve as a detailed step-by-step practical guide through preparing, processing, and analyzing fiber photometry data with the PASTa protocol.","title":"PASTa User Guide"}]}