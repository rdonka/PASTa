{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"PASTa: Photometry Analysis and Signal Processing Toolbox Welcome to the documentation site for PASTa (Photometry Analysis and Signal Processing Toolbox)! The PASTa protocol is an open source toolbox and protocol for the preparation, signal processing, and analysis of fiber photometry data. Fiber photometry is a rapidly growing technique to record real-time neural signaling in awake, behaving subjects. However, the processing and analysis of photometry data streams can be complicated, and there is wide divergence in methods across the field. While several opensource signal processing tools exist, platforms can be inflexible in accommodating experimental designs, lack consistency in peak detection, and be difficult for naive users. The PASTa protocol is developed to be highly flexible and adaptable to a wide variety of experimental designs. While operating through MATLAB, the code is annotated to be readable, accessible, and adaptable for new users. Here you'll find set up instructions, a detailed user guide, example analyses, and additional details on function inputs and usage. For the toolbox and code repository, visit our GitHub .","title":"PASTa: Photometry Analysis and Signal Processing Toolbox"},{"location":"#pasta-photometry-analysis-and-signal-processing-toolbox","text":"Welcome to the documentation site for PASTa (Photometry Analysis and Signal Processing Toolbox)! The PASTa protocol is an open source toolbox and protocol for the preparation, signal processing, and analysis of fiber photometry data. Fiber photometry is a rapidly growing technique to record real-time neural signaling in awake, behaving subjects. However, the processing and analysis of photometry data streams can be complicated, and there is wide divergence in methods across the field. While several opensource signal processing tools exist, platforms can be inflexible in accommodating experimental designs, lack consistency in peak detection, and be difficult for naive users. The PASTa protocol is developed to be highly flexible and adaptable to a wide variety of experimental designs. While operating through MATLAB, the code is annotated to be readable, accessible, and adaptable for new users. Here you'll find set up instructions, a detailed user guide, example analyses, and additional details on function inputs and usage. For the toolbox and code repository, visit our GitHub .","title":"PASTa: Photometry Analysis and Signal Processing Toolbox"},{"location":"about/","text":"About PASTa was developed by the Roitman laboratories. For more information, or to request new features feel free to reach out! Dr. Jamie Roitman: https://jamiedroitman.wixsite.com/roitmandecisionlab Dr. Mitch Roitman: https://www.mroitmanlab.com/","title":"About"},{"location":"about/#about","text":"PASTa was developed by the Roitman laboratories. For more information, or to request new features feel free to reach out! Dr. Jamie Roitman: https://jamiedroitman.wixsite.com/roitmandecisionlab Dr. Mitch Roitman: https://www.mroitmanlab.com/","title":"About"},{"location":"exampleanalyses/","text":"To help users get started with the PASTa Protocol, here some example analyses are included with raw data to walk users through each step of the process. Injection Transients This example is of whole session transient analysis to determine changes in dopaminergic transients following morphine injection. Data Summary Example data are provided from two subjects. Each subject has two fiber photometry sessions conducted on consecutive days: saline and morphine (10mg/kg). Each session consists of a fifteen minute baseline, i.p. injection marked by epocs sent through Med Associates equipment and stored by Synapse, and a 60 minute post injection recording period. The example analysis script is located in the repository under the folder Example Analyses. Data are available to download on Box here. The subfolder Injection Transients is prepared for the analysis with folders created for extracted data, analysis, and figure outputs. Raw data blocks collected via Synapse (Tucker Davis Technologies) are nested in the Raw Data folder. If you have any questions or run into problems accessing the files, please feel free to reach out. Data Preparation The first section of the script sets up paths and analysis key inputs. To enable users to switch computers easily, paths are created without the computer and user specific portion. The computer user specific portion of the path is input to the variable computeruserpath and appended to subsequently needed paths. To access files and functions, paths need to be added to MATLAB via the addpath function. genpath is used within addpath to ensure folders and subfolders at the input path are added. Finally, the names of created Subject and File Keys are added to variables subjectkeyname and filekeyname. These keys contain the session specific and subject specific information needed to load the data and analyze the results. For details on keys, see the user guide section on data preparation. The keys are loaded using loadKeys to join the subject specific data to the session specific information contained in the file key. Paths to raw data blocks and extracted output locations are added to string arrays, which are input to the function extractFPdata to extract and save raw data as MATLAB structs. Extracted structures are loaded by the function loadKeydata. To trim excess samples before the start of the program and after the end of the post injection period, indices are prepared for each file and the function trimFPdata is used to adjust the streams. Signal Processing To control for motion artifacts and photobleaching, the 405 nm channel (baq) is subtracted from the 465 nm channel (sig) with the function subtractFPdata. The function is called twice to output both the subtracted (sigsub) as well as the subtracted and filtered signal (sigfilt). The scaled 405 nm background channel is also output as _baqsub _and _baqfilt _respectively. After subtraction, sigfilt is normalized. To normalize to the entire session, the function normSession is called, outputting both df/f (sig_normsession_df) and z scored signal streams (sig_normsession_z). To normalize to pre injection baseline, the function normBaseline is called, which uses the pre-injection period to normalize the entire session, also outputting df/f (sig_normbl_df) and z scored (sig_normbl_z) streams. Finally, a for loop is used to create trimmed streams with the injection time period removed. For details on the signal processing functions, see Wiki section 4. Signal Processing. Transient Detection To identify transient events, the findSessionTransients functions are used. These functions detect relevant transients based on an amplitude inclusiong criterion (eg, 3SD amplitude) relative to baseline. Multiple baseline options exist - see function documentation for findSessionTransients_blmin, findSessionTransients_blmean, and findSessionTransients_localmin. After detection, transients are quantified by frequency, amplitude, half height rise time, half height fall time, and halfheight AUC. After detection and quantification, transients are binned into 5 minute increments. To do so, the number of samples per bin is identified and added to the data structure. The function binSessionTransients identifies the bin of each transient event in the table output by findSessionTransients. To export the transient events for statistical analysis, the table for each file is appended into the table alltransients which is output as a csv to the location specified by analysispath. Plots First, full session traces are plotted for all relevant streams. In this example, the raw 465 (sig), 405 (baq), subtracted signal (sigsub), subtracted and filtered signal (sigfilt), and baseline normalized signal (sig_normbl_z) are plotted. The function plotTraces is used within a for loop by file to generate plots for each session. plotTraces outputs a tiled plot object (alltraces) with each stream plot as a tile. The tiles are modified to add markers for injection start and end. A main title is also added to the full plot. The plot is saved by subject and injection type into the figure folder specified by the figurepath.","title":"Example Analyses"},{"location":"exampleanalyses/#injection-transients","text":"This example is of whole session transient analysis to determine changes in dopaminergic transients following morphine injection.","title":"Injection Transients"},{"location":"exampleanalyses/#data-summary","text":"Example data are provided from two subjects. Each subject has two fiber photometry sessions conducted on consecutive days: saline and morphine (10mg/kg). Each session consists of a fifteen minute baseline, i.p. injection marked by epocs sent through Med Associates equipment and stored by Synapse, and a 60 minute post injection recording period. The example analysis script is located in the repository under the folder Example Analyses. Data are available to download on Box here. The subfolder Injection Transients is prepared for the analysis with folders created for extracted data, analysis, and figure outputs. Raw data blocks collected via Synapse (Tucker Davis Technologies) are nested in the Raw Data folder. If you have any questions or run into problems accessing the files, please feel free to reach out.","title":"Data Summary"},{"location":"exampleanalyses/#data-preparation","text":"The first section of the script sets up paths and analysis key inputs. To enable users to switch computers easily, paths are created without the computer and user specific portion. The computer user specific portion of the path is input to the variable computeruserpath and appended to subsequently needed paths. To access files and functions, paths need to be added to MATLAB via the addpath function. genpath is used within addpath to ensure folders and subfolders at the input path are added. Finally, the names of created Subject and File Keys are added to variables subjectkeyname and filekeyname. These keys contain the session specific and subject specific information needed to load the data and analyze the results. For details on keys, see the user guide section on data preparation. The keys are loaded using loadKeys to join the subject specific data to the session specific information contained in the file key. Paths to raw data blocks and extracted output locations are added to string arrays, which are input to the function extractFPdata to extract and save raw data as MATLAB structs. Extracted structures are loaded by the function loadKeydata. To trim excess samples before the start of the program and after the end of the post injection period, indices are prepared for each file and the function trimFPdata is used to adjust the streams.","title":"Data Preparation"},{"location":"exampleanalyses/#signal-processing","text":"To control for motion artifacts and photobleaching, the 405 nm channel (baq) is subtracted from the 465 nm channel (sig) with the function subtractFPdata. The function is called twice to output both the subtracted (sigsub) as well as the subtracted and filtered signal (sigfilt). The scaled 405 nm background channel is also output as _baqsub _and _baqfilt _respectively. After subtraction, sigfilt is normalized. To normalize to the entire session, the function normSession is called, outputting both df/f (sig_normsession_df) and z scored signal streams (sig_normsession_z). To normalize to pre injection baseline, the function normBaseline is called, which uses the pre-injection period to normalize the entire session, also outputting df/f (sig_normbl_df) and z scored (sig_normbl_z) streams. Finally, a for loop is used to create trimmed streams with the injection time period removed. For details on the signal processing functions, see Wiki section 4. Signal Processing.","title":"Signal Processing"},{"location":"exampleanalyses/#transient-detection","text":"To identify transient events, the findSessionTransients functions are used. These functions detect relevant transients based on an amplitude inclusiong criterion (eg, 3SD amplitude) relative to baseline. Multiple baseline options exist - see function documentation for findSessionTransients_blmin, findSessionTransients_blmean, and findSessionTransients_localmin. After detection, transients are quantified by frequency, amplitude, half height rise time, half height fall time, and halfheight AUC. After detection and quantification, transients are binned into 5 minute increments. To do so, the number of samples per bin is identified and added to the data structure. The function binSessionTransients identifies the bin of each transient event in the table output by findSessionTransients. To export the transient events for statistical analysis, the table for each file is appended into the table alltransients which is output as a csv to the location specified by analysispath.","title":"Transient Detection"},{"location":"exampleanalyses/#plots","text":"First, full session traces are plotted for all relevant streams. In this example, the raw 465 (sig), 405 (baq), subtracted signal (sigsub), subtracted and filtered signal (sigfilt), and baseline normalized signal (sig_normbl_z) are plotted. The function plotTraces is used within a for loop by file to generate plots for each session. plotTraces outputs a tiled plot object (alltraces) with each stream plot as a tile. The tiles are modified to add markers for injection start and end. A main title is also added to the full plot. The plot is saved by subject and injection type into the figure folder specified by the figurepath.","title":"Plots"},{"location":"functiondocumentation/","text":"Function Documentation Overview This page contains additional documentation for each function within PASTa, as well as examples of inputs. Data Preparation Functions This set of functions is used to prepare raw photometry data, match it with experimental metadata, and load data into a structure in MATLAB. Functions are provided to handle data collected via TDT equipment and software Synapse, or a generic file structure with data streams saved to CSV files. loadKeys Combines subject key and file key into a data structure, and appends the provided computeruserpath to the paths in the file key. INPUTS: COMPUTERUSERPATH: A variable containing the unique portion of the file explorer path for the users specific computer. For example, 'C:\\Users\\rmdon\\'. Make sure the computeruserpath ends in a forward slash. SUBJECTKEYNAME: A variable containing a string with the name of the subject key csv file for the experiment (see \"3. Data Analysis Pipeline\" for more details). To omit a subject key and only load in the file key, set subjectkeyname = \"\". FILEKEYNAME: A variable containing a string with the name of the file key csv file for the experiment (see \"3. Data Analysis Pipeline\" for more details). OUTPUTS: EXPERIMENTKEY: A data structure called \"experimentkey\" that includes the joined file key and subject key with the computer user path appended to raw and extracted folder paths. EXAMPLE: computeruserpath = 'C:\\Users\\MYNAME\\'; % Computer specific portion of file navigation paths subjectkeyname = 'Subject Key.csv'; % Name of csv file containing subject information; set to '' if not using a subject key filekeyname = 'File Key.csv'; % Name of csv file containing session information, raw data folder names, and paths [experimentkey] = loadKeys(computeruserpath, subjectkeyname, filekeyname); % Load keys into a data structure called experimentkey NOTES: FILEKEY must contain at a minimum the fields Subject , RawFolderPath , and ExtractedFolderPath . SUBJECTKEY must contain at a minimum the field Subject Folder paths must end with a slash. The subject and file keys are joined based on Subject ID. Subject key must contain every subjects in the file key. If there is a mismatch, you will receive an error message that the right table does not contain all the key variables that are in the left table. The error message will display the unique subject IDs present in each key so you can determine where the mismatch occurred. Fields in subject and file key must be named uniquely. The only field that should be named the same in both keys is Subject. extractTDTdata This function is used to extract TDT data from saved blocks recorded via the software Synapse . For each block, extractTDTdata calls the function \"TDTbin2mat\" (TDT, 2019) and inputs the RawFolderPath to extract fiber photometry data recorded with Synapse. Extracted blocks are parsed it into a single data structure containing all fields, streams, and epocs. The function will identify the signal channel by matching the names in the input SIGSTREAMNAMES and the control channel by matching the names in the input BAQSTREAMNAMES. The name inputs can include a list of stream names if channel naming conventions vary by rig. Each block is saved as a separate data structure in a '.mat' file at the location specified by the inputs in extractedfolderpaths. INPUTS: RAWFOLDERPATHS: a string array containing the paths to the folder location of the raw data blocks to be extracted. The string array should contain one column with each full path in a separate row. EXTRACTEDFOLDERPATHS: a string array containing the paths to the folder location in which to save the extracted MatLab structs for each block to be extracted. The string array should contain one column with each full path in a separate row. SIGSTREAMNAMES: A cell array containing strings with the names of the streams to be treated as signal. Note that only one stream per file can be treated as signal. If different files have different stream names, include all stream names in the cell array. BAQSTREAMNAMES: A cell array containing strings with the names of the streams to be treated as background. Note that only one stream per file can be treated as background. If different files have different stream names, include all stream names in the cell array. OPTIONAL INPUTS: CLIP: the number of seconds to remove on either end of the data streams. If not specified, defaults to 5 seconds. SKIPEXISTING: A binary variable containing a 0 if pre-existing extracted blocks should be re-extracted or a 1 if pre-existing extracted blocks should be skipped. This allows the user to toggle whether or not to extract every block, or only blocks that have not previously been extracted. If not specified, defaults to 1 (skip previously extracted blocks). OUTPUTS: Saved .mat data structures for each block in the location specified by extractedfolderpaths. EXAMPLE - DEFAULT: sigstreamnames = {'x65A', '465A'}; % All names of signal streams across files baqstreamnames = {'x05A', '405A'}; % All names of background streams across files rawfolderpaths = string({experimentkey.RawFolderPath})'; % Create string array of raw folder paths extractedfolderpaths = string({experimentkey.ExtractedFolderPath})'; % Create string array of extracted folder paths extractTDTdata(rawfolderpaths,extractedfolderpaths,sigstreamnames,baqstreamnames,clip,skipexisting); % extract data EXAMPLE - MANUALLY SPECIFIED CLIP AND SKIPEXISTING: clip = 3; skipexisting = 0; sigstreamnames = {'x65A', '465A'}; % All names of signal streams across files baqstreamnames = {'x05A', '405A'}; % All names of background streams across files rawfolderpaths = string({experimentkey.RawFolderPath})'; % Create string array of raw folder paths' extractedfolderpaths = string({experimentkey.ExtractedFolderPath})'; % Create string array of extracted folder paths' extractTDTdata(rawfolderpaths,extractedfolderpaths,sigstreamnames,baqstreamnames,'clip',clip,'skipexisting',skipexisting); % extract data loadTDTdata For use with previously extracted data collect with TDT equipment and software Synapse. loadTDTData loads previously extracted .mat data blocks into a data structure for further analysis. Each block is one row. The input experiment key must include at a minimum the extracted folder path location of the block. Any additional information about the subject and session in the experiment key will be matched to the extracted data. INPUTS: EXPERIMENTKEY: A prepared data structure with at minimum the ExtractedFolderPath to locate the individual block structures to be loaded. OUTPUTS: DATA: the input data structure with each individual extracted block added by row. EXAMPLE: [rawdata] = loadKeydata(experimentkey); % Load data based on the experiment key into the structure 'rawdata' loadCSVdata For use with data collected and stored to a general file structure. coming soon trimFPdata Used to trim samples at the very start and end of recordings that are not to be included in analysis (such as the the first two minutes of the session, or the first samples before a hardware control program is initiated). Trims all specified data streams from the index in trimstart to the index in trimend, and adjusts epocs by the amount trimmed by trimstart. Users must pre-prepare the trim start and end indexes to specify as inputs for the function. INPUTS: DATA: A data frame containing at least the specified input fields. TRIMSTART: The location to start trimming at. TRIMEND: The location to end trimming at. WHICHSTREAMS: A cell array containing the names of all the streams to be trimmed. OPTIONAL INPUTS: WHICHEPOCS: A cell array containing the names of all the epocs to be adjusted due to trimming - subtract the (start loc - 1) from the epoc. OUTPUTS: DATA: The data structure with the specified data stream containing the trimmed data. EXAMPLE: trimstart = 'sessionstart'; % name of field with session start index trimend = 'sessionend'; % name of field with session end index whichstreams = {'sig', 'baq','time'}; % which streams to trim whichepocs = {'injt','sess'}; % which epocs to adjust to maintain relative position [data] = trimFPdata(rawdata,trimstart,trimend, whichstreams,whichepocs); % Output trimmed data into new structure called data Signal Processing Functions subtractFPdata Used to subtract the background photometry stream (eg, 405nm) from the signal stream (eg, 465nm), convert the subtracted signal to delta F/f, and apply a filter to denoise the output. Users must input the data structure with the raw data, the names of the fields containing the signal and background streams, and the sampling rate of the collected data. INPUTS: DATA: A data frame containing at least the specified input fields. SIGFIELD: The name (string) of the field containing the signal stream. BAQFIELD: The name (string) of the field containing the background stream. FS: The sampling rate of the raw data collection in hz. OPTIONAL INPUTS: BAQSCALINGTYPE: A string to specify the type of background scaling to apply. Options are 'frequency', 'sigmean', 'OLS', 'detrendOLS', 'smoothedOLS', or 'IRLS'. Default: 'frequency'. 'frequency': Scales the background to the signal channel based on ratio of specified frequency bands in the FFT (frequency domain) of the channels. 'sigmean': Scales the background to the signal channel based on the ratio of the mean of the signal to the mean of the background (time domain). 'OLS': Uses ordinary least-squares regression to generate scaled background. 'detrendOLS': Removes the linear trend from signal and background streams prior to using ordinary least-squares regression to generate scaled background. 'smoothedOLS': Applies lowess smoothing to the signal and background streams prior to using ordinary least-squares regression to generate scaled background. 'IRLS': Uses iteratively reweighted least squares regression to generate scaled background. BAQSCALINGFREQ: Only used with 'frequency' scaling. Numeric frequency (Hz) threshold for scaling the background to signal channel. Frequencies above this value will be included in the scaling factor determination. Default: 10 Hz. BAQSCALINGPERC: Only used with 'frequency' and 'sigmean' scaling. Adjusts the background scaling factor to be a percednt of the derived scaling factor value. Default: 1 (100%). SUBTRACTIONOUTPUT: Output type for the subtracted data. Default: 'dff' 'dff': Outputs subtracted signal as delta F/F. 'df': Outputs subtracted signal as delta F. FILTERTYPE: A string to specify the type of filter to apply after subtraction. Default: 'bandpass'. 'nofilter': No filter will be applied. 'bandpass': A bandpass filter will be applied. 'highpass': Only the high pass filter will be applied. 'lowpass': Only the low pass filter will be applied. PADDING: Defaults to 1, which applies padding. Padding takes the first 10% of the stream, flips it, and appends it to the data before filtering. Appended data is trimmed after filtration. Set to 0 to turn off padding of data streams. Default: 1. PADDINGPERC: Percent of data length to use to determine the number of samples to be appended to the beginning and end of data in padding. Set to minimum 10%. Default: 0.1 (10%). FILTERORDER: The order to be used for the chosen butterworth filter. Default: 3. HIGHPASSCUTOFF: The cutoff frequency (hz) to be used for the high pass butterworth filter. Default: 2.2860. LOWPASSCUTOFF: The cutoff to be used for the low pass butterworth filter. Default: 0.0051. NOTE: 'bandpass' applies both the high and low cutoffs to design the filter. SUPRESSDISP: If set to anything other than 0, this will suppress the command window displays. Default: 0. OUTPUTS: DATA: The original data structure with added fields with the scaled background ('baq_scaled'), subtracted signal ('sigsub'), and subtracted and filtered signal ('sigfilt'). All inputs and defaults will be added to the data structure under the field 'inputs'. NOTE: If using BAQSCALINGMETHOD 'detrendOLS', additional fields containing the detrended signal and background ('sig_detrend' and 'baq_detrend') will be added to the data frame. If using BAQSCALINGMETHOD 'smoothedOLS', additional fields containing the smoothed signal and background ('sig_smoothed' and 'baq_smoothed') will be added to the data frame. EXAMPLE - DEFAULT: sigfield = 'sig'; baqfield = 'baq'; fs = 1017; data = subtractFPdata(data,sigfield,baqfield,fs); EXAMPLE - Frequency Scaling with 20Hz Threshold and Highpass Filter Only: sigfield = 'sig'; baqfield = 'baq'; fs = 1017; data = subtractFPdata(data,sigfield,baqfield,fs,'baqscalingfreq',20,'filtertype,'highpass'); Normalization Peak Detection and Quantification Functions findpeaks peakarea Individual Trial Analyses Functions cutTrialData trialAverage trialnormalization Plotting Functions Whole Session Plots FFT Plots Peak Plots Trial Plots","title":"Function Documentation"},{"location":"functiondocumentation/#function-documentation-overview","text":"This page contains additional documentation for each function within PASTa, as well as examples of inputs.","title":"Function Documentation Overview"},{"location":"functiondocumentation/#data-preparation-functions","text":"This set of functions is used to prepare raw photometry data, match it with experimental metadata, and load data into a structure in MATLAB. Functions are provided to handle data collected via TDT equipment and software Synapse, or a generic file structure with data streams saved to CSV files.","title":"Data Preparation Functions"},{"location":"functiondocumentation/#loadkeys","text":"Combines subject key and file key into a data structure, and appends the provided computeruserpath to the paths in the file key. INPUTS: COMPUTERUSERPATH: A variable containing the unique portion of the file explorer path for the users specific computer. For example, 'C:\\Users\\rmdon\\'. Make sure the computeruserpath ends in a forward slash. SUBJECTKEYNAME: A variable containing a string with the name of the subject key csv file for the experiment (see \"3. Data Analysis Pipeline\" for more details). To omit a subject key and only load in the file key, set subjectkeyname = \"\". FILEKEYNAME: A variable containing a string with the name of the file key csv file for the experiment (see \"3. Data Analysis Pipeline\" for more details). OUTPUTS: EXPERIMENTKEY: A data structure called \"experimentkey\" that includes the joined file key and subject key with the computer user path appended to raw and extracted folder paths. EXAMPLE: computeruserpath = 'C:\\Users\\MYNAME\\'; % Computer specific portion of file navigation paths subjectkeyname = 'Subject Key.csv'; % Name of csv file containing subject information; set to '' if not using a subject key filekeyname = 'File Key.csv'; % Name of csv file containing session information, raw data folder names, and paths [experimentkey] = loadKeys(computeruserpath, subjectkeyname, filekeyname); % Load keys into a data structure called experimentkey NOTES: FILEKEY must contain at a minimum the fields Subject , RawFolderPath , and ExtractedFolderPath . SUBJECTKEY must contain at a minimum the field Subject Folder paths must end with a slash. The subject and file keys are joined based on Subject ID. Subject key must contain every subjects in the file key. If there is a mismatch, you will receive an error message that the right table does not contain all the key variables that are in the left table. The error message will display the unique subject IDs present in each key so you can determine where the mismatch occurred. Fields in subject and file key must be named uniquely. The only field that should be named the same in both keys is Subject.","title":"loadKeys"},{"location":"functiondocumentation/#extracttdtdata","text":"This function is used to extract TDT data from saved blocks recorded via the software Synapse . For each block, extractTDTdata calls the function \"TDTbin2mat\" (TDT, 2019) and inputs the RawFolderPath to extract fiber photometry data recorded with Synapse. Extracted blocks are parsed it into a single data structure containing all fields, streams, and epocs. The function will identify the signal channel by matching the names in the input SIGSTREAMNAMES and the control channel by matching the names in the input BAQSTREAMNAMES. The name inputs can include a list of stream names if channel naming conventions vary by rig. Each block is saved as a separate data structure in a '.mat' file at the location specified by the inputs in extractedfolderpaths. INPUTS: RAWFOLDERPATHS: a string array containing the paths to the folder location of the raw data blocks to be extracted. The string array should contain one column with each full path in a separate row. EXTRACTEDFOLDERPATHS: a string array containing the paths to the folder location in which to save the extracted MatLab structs for each block to be extracted. The string array should contain one column with each full path in a separate row. SIGSTREAMNAMES: A cell array containing strings with the names of the streams to be treated as signal. Note that only one stream per file can be treated as signal. If different files have different stream names, include all stream names in the cell array. BAQSTREAMNAMES: A cell array containing strings with the names of the streams to be treated as background. Note that only one stream per file can be treated as background. If different files have different stream names, include all stream names in the cell array. OPTIONAL INPUTS: CLIP: the number of seconds to remove on either end of the data streams. If not specified, defaults to 5 seconds. SKIPEXISTING: A binary variable containing a 0 if pre-existing extracted blocks should be re-extracted or a 1 if pre-existing extracted blocks should be skipped. This allows the user to toggle whether or not to extract every block, or only blocks that have not previously been extracted. If not specified, defaults to 1 (skip previously extracted blocks). OUTPUTS: Saved .mat data structures for each block in the location specified by extractedfolderpaths. EXAMPLE - DEFAULT: sigstreamnames = {'x65A', '465A'}; % All names of signal streams across files baqstreamnames = {'x05A', '405A'}; % All names of background streams across files rawfolderpaths = string({experimentkey.RawFolderPath})'; % Create string array of raw folder paths extractedfolderpaths = string({experimentkey.ExtractedFolderPath})'; % Create string array of extracted folder paths extractTDTdata(rawfolderpaths,extractedfolderpaths,sigstreamnames,baqstreamnames,clip,skipexisting); % extract data EXAMPLE - MANUALLY SPECIFIED CLIP AND SKIPEXISTING: clip = 3; skipexisting = 0; sigstreamnames = {'x65A', '465A'}; % All names of signal streams across files baqstreamnames = {'x05A', '405A'}; % All names of background streams across files rawfolderpaths = string({experimentkey.RawFolderPath})'; % Create string array of raw folder paths' extractedfolderpaths = string({experimentkey.ExtractedFolderPath})'; % Create string array of extracted folder paths' extractTDTdata(rawfolderpaths,extractedfolderpaths,sigstreamnames,baqstreamnames,'clip',clip,'skipexisting',skipexisting); % extract data","title":"extractTDTdata"},{"location":"functiondocumentation/#loadtdtdata","text":"For use with previously extracted data collect with TDT equipment and software Synapse. loadTDTData loads previously extracted .mat data blocks into a data structure for further analysis. Each block is one row. The input experiment key must include at a minimum the extracted folder path location of the block. Any additional information about the subject and session in the experiment key will be matched to the extracted data. INPUTS: EXPERIMENTKEY: A prepared data structure with at minimum the ExtractedFolderPath to locate the individual block structures to be loaded. OUTPUTS: DATA: the input data structure with each individual extracted block added by row. EXAMPLE: [rawdata] = loadKeydata(experimentkey); % Load data based on the experiment key into the structure 'rawdata'","title":"loadTDTdata"},{"location":"functiondocumentation/#loadcsvdata","text":"For use with data collected and stored to a general file structure. coming soon","title":"loadCSVdata"},{"location":"functiondocumentation/#trimfpdata","text":"Used to trim samples at the very start and end of recordings that are not to be included in analysis (such as the the first two minutes of the session, or the first samples before a hardware control program is initiated). Trims all specified data streams from the index in trimstart to the index in trimend, and adjusts epocs by the amount trimmed by trimstart. Users must pre-prepare the trim start and end indexes to specify as inputs for the function. INPUTS: DATA: A data frame containing at least the specified input fields. TRIMSTART: The location to start trimming at. TRIMEND: The location to end trimming at. WHICHSTREAMS: A cell array containing the names of all the streams to be trimmed. OPTIONAL INPUTS: WHICHEPOCS: A cell array containing the names of all the epocs to be adjusted due to trimming - subtract the (start loc - 1) from the epoc. OUTPUTS: DATA: The data structure with the specified data stream containing the trimmed data. EXAMPLE: trimstart = 'sessionstart'; % name of field with session start index trimend = 'sessionend'; % name of field with session end index whichstreams = {'sig', 'baq','time'}; % which streams to trim whichepocs = {'injt','sess'}; % which epocs to adjust to maintain relative position [data] = trimFPdata(rawdata,trimstart,trimend, whichstreams,whichepocs); % Output trimmed data into new structure called data","title":"trimFPdata"},{"location":"functiondocumentation/#signal-processing-functions","text":"","title":"Signal Processing Functions"},{"location":"functiondocumentation/#subtractfpdata","text":"Used to subtract the background photometry stream (eg, 405nm) from the signal stream (eg, 465nm), convert the subtracted signal to delta F/f, and apply a filter to denoise the output. Users must input the data structure with the raw data, the names of the fields containing the signal and background streams, and the sampling rate of the collected data. INPUTS: DATA: A data frame containing at least the specified input fields. SIGFIELD: The name (string) of the field containing the signal stream. BAQFIELD: The name (string) of the field containing the background stream. FS: The sampling rate of the raw data collection in hz. OPTIONAL INPUTS: BAQSCALINGTYPE: A string to specify the type of background scaling to apply. Options are 'frequency', 'sigmean', 'OLS', 'detrendOLS', 'smoothedOLS', or 'IRLS'. Default: 'frequency'. 'frequency': Scales the background to the signal channel based on ratio of specified frequency bands in the FFT (frequency domain) of the channels. 'sigmean': Scales the background to the signal channel based on the ratio of the mean of the signal to the mean of the background (time domain). 'OLS': Uses ordinary least-squares regression to generate scaled background. 'detrendOLS': Removes the linear trend from signal and background streams prior to using ordinary least-squares regression to generate scaled background. 'smoothedOLS': Applies lowess smoothing to the signal and background streams prior to using ordinary least-squares regression to generate scaled background. 'IRLS': Uses iteratively reweighted least squares regression to generate scaled background. BAQSCALINGFREQ: Only used with 'frequency' scaling. Numeric frequency (Hz) threshold for scaling the background to signal channel. Frequencies above this value will be included in the scaling factor determination. Default: 10 Hz. BAQSCALINGPERC: Only used with 'frequency' and 'sigmean' scaling. Adjusts the background scaling factor to be a percednt of the derived scaling factor value. Default: 1 (100%). SUBTRACTIONOUTPUT: Output type for the subtracted data. Default: 'dff' 'dff': Outputs subtracted signal as delta F/F. 'df': Outputs subtracted signal as delta F. FILTERTYPE: A string to specify the type of filter to apply after subtraction. Default: 'bandpass'. 'nofilter': No filter will be applied. 'bandpass': A bandpass filter will be applied. 'highpass': Only the high pass filter will be applied. 'lowpass': Only the low pass filter will be applied. PADDING: Defaults to 1, which applies padding. Padding takes the first 10% of the stream, flips it, and appends it to the data before filtering. Appended data is trimmed after filtration. Set to 0 to turn off padding of data streams. Default: 1. PADDINGPERC: Percent of data length to use to determine the number of samples to be appended to the beginning and end of data in padding. Set to minimum 10%. Default: 0.1 (10%). FILTERORDER: The order to be used for the chosen butterworth filter. Default: 3. HIGHPASSCUTOFF: The cutoff frequency (hz) to be used for the high pass butterworth filter. Default: 2.2860. LOWPASSCUTOFF: The cutoff to be used for the low pass butterworth filter. Default: 0.0051. NOTE: 'bandpass' applies both the high and low cutoffs to design the filter. SUPRESSDISP: If set to anything other than 0, this will suppress the command window displays. Default: 0. OUTPUTS: DATA: The original data structure with added fields with the scaled background ('baq_scaled'), subtracted signal ('sigsub'), and subtracted and filtered signal ('sigfilt'). All inputs and defaults will be added to the data structure under the field 'inputs'. NOTE: If using BAQSCALINGMETHOD 'detrendOLS', additional fields containing the detrended signal and background ('sig_detrend' and 'baq_detrend') will be added to the data frame. If using BAQSCALINGMETHOD 'smoothedOLS', additional fields containing the smoothed signal and background ('sig_smoothed' and 'baq_smoothed') will be added to the data frame. EXAMPLE - DEFAULT: sigfield = 'sig'; baqfield = 'baq'; fs = 1017; data = subtractFPdata(data,sigfield,baqfield,fs); EXAMPLE - Frequency Scaling with 20Hz Threshold and Highpass Filter Only: sigfield = 'sig'; baqfield = 'baq'; fs = 1017; data = subtractFPdata(data,sigfield,baqfield,fs,'baqscalingfreq',20,'filtertype,'highpass');","title":"subtractFPdata"},{"location":"functiondocumentation/#normalization","text":"","title":"Normalization"},{"location":"functiondocumentation/#peak-detection-and-quantification-functions","text":"","title":"Peak Detection and Quantification Functions"},{"location":"functiondocumentation/#findpeaks","text":"","title":"findpeaks"},{"location":"functiondocumentation/#peakarea","text":"","title":"peakarea"},{"location":"functiondocumentation/#individual-trial-analyses-functions","text":"","title":"Individual Trial Analyses Functions"},{"location":"functiondocumentation/#cuttrialdata","text":"","title":"cutTrialData"},{"location":"functiondocumentation/#trialaverage","text":"","title":"trialAverage"},{"location":"functiondocumentation/#trialnormalization","text":"","title":"trialnormalization"},{"location":"functiondocumentation/#plotting-functions","text":"","title":"Plotting Functions"},{"location":"functiondocumentation/#whole-session-plots","text":"","title":"Whole Session Plots"},{"location":"functiondocumentation/#fft-plots","text":"","title":"FFT Plots"},{"location":"functiondocumentation/#peak-plots","text":"","title":"Peak Plots"},{"location":"functiondocumentation/#trial-plots","text":"","title":"Trial Plots"},{"location":"gettingstarted/","text":"Welcome to the PASTa Quick Start Guide! PASTa (Photometry Analysis and Signal Processing Toolbox) is a MATLAB toolbox developed for signal processing anad analysis of fiber photometry data. The toolbox is written to be user-friendly and approachable for people at all levels of coding, with flexible functions to make analysis of a wide variety of experimental designs feasible. Functions are included for data management and organization, signal processing, and experimental analyses including whole session and event/trial specific designs. Here you'll find the quick start guide to PASTa with instructions on set up and installation, and instructions for basic use of the toolbox. For full documentation visit mkdocs.org . Set Up and Installation GitHub and GitHub Desktop Install GitHub Desktop if you haven't already. Make an account on GitHub, note that students get access to premium features for free through the student developer pack once verified. Install GitHub Desktop and set up a location for your repositories on your local computer. For newer users, we recommend making a folder on your desktop called \"GitHubRepositories\" and cloning all repositories there. This makes it very easy to locate your repositories. For an overview of how to use GitHub and best practices: https://www.freecodecamp.org/news/introduction-to-git-and-github/ MATLAB Install MATLAB to you computer, including the simulink suite. Functions have been tested with MALTAB 2022, 2023, and 2024 releases.","title":"Getting Started"},{"location":"gettingstarted/#welcome-to-the-pasta-quick-start-guide","text":"PASTa (Photometry Analysis and Signal Processing Toolbox) is a MATLAB toolbox developed for signal processing anad analysis of fiber photometry data. The toolbox is written to be user-friendly and approachable for people at all levels of coding, with flexible functions to make analysis of a wide variety of experimental designs feasible. Functions are included for data management and organization, signal processing, and experimental analyses including whole session and event/trial specific designs. Here you'll find the quick start guide to PASTa with instructions on set up and installation, and instructions for basic use of the toolbox. For full documentation visit mkdocs.org .","title":"Welcome to the PASTa Quick Start Guide!"},{"location":"gettingstarted/#set-up-and-installation","text":"","title":"Set Up and Installation"},{"location":"gettingstarted/#github-and-github-desktop","text":"Install GitHub Desktop if you haven't already. Make an account on GitHub, note that students get access to premium features for free through the student developer pack once verified. Install GitHub Desktop and set up a location for your repositories on your local computer. For newer users, we recommend making a folder on your desktop called \"GitHubRepositories\" and cloning all repositories there. This makes it very easy to locate your repositories. For an overview of how to use GitHub and best practices: https://www.freecodecamp.org/news/introduction-to-git-and-github/","title":"GitHub and GitHub Desktop"},{"location":"gettingstarted/#matlab","text":"Install MATLAB to you computer, including the simulink suite. Functions have been tested with MALTAB 2022, 2023, and 2024 releases.","title":"MATLAB"},{"location":"userguide/datapreparation/","text":"Data Preparation The first stage in the PASTa Protocol is data preparation, which involves associating fiber photometry data with experimental meta data, loading raw data, and extracting and saving the raw data as MATLAB data structures to faciliate future analysis sessions. For data collected with Tucker Davis Technologies equipment and software Synapse , custom functions are included to extract data from raw formats. For data collected with other systems, data must be pre-formatted to match a generic csv file format structure and loaded with provided custom functions, or prepared separately by the user. If your data doesn't match the available load options, please feel free to reach out! Data Organization The PASTa protocol is set up to accomodate any kind of file organization preferred by the user. Organization preference may vary depending on your lab's file storage practices, photometry equipment, or individual projects. TDT Synapse Output Fiber photometry data collected through the software Synapse (Tucker Davis Technologies) is stored in tanks and blocks . Tanks are parent folders created by Synapse for each experiment. Blocks are individual folders for each session containing the actual output data. Stored data cannot be accessed directly via the folder, but rather must be extracted via MATLAB. By default, the tank path is: C:\\TDT\\Synapse\\Tanks. Synapse recognizes experiments and subjects as key categories of information that play a special role in managing data storage and retrieval. Thus, when running the session, it is critical to ensure the correct Experiment profile is selected, and the correct Subject identifier is input for each session. By default, Synapse names data tanks automatically based on experiment name and the start time of the first recording {ExperimentName}-{yymmdd}-{hhmmss}. Blocks of data are named based on subject {SubjectName}-{yymmdd}-{hhmmss} for each recording session and the start time. Synapse will save a new Tank for every day unless you change the default setting. Click Menu at the top of the bar, then Preferences. Under the Data Saving tab, make sure \"New Tank Each Day\" is unchecked. Experiment Key Creation To accomodate a variety of file organization structures, users can first create two csv files containing the information necessary to access raw data, and experimental metadata to match to raw photometry data. MATLAB can access raw data folders stored either locally or in a cloud-based storage app like Box or Dropbox. To faciliate analysis, users can create two keys as csv files: a subject key and a file key. In the PASTa protocol, these files will be knit together to pair subject specific information with each individual session of data, preventing the need for manual repeated entry of subject specific information and reduce the time burden of properly maintaining and including experimental metadata factors like subject traits, treatments, and experimental equipment. First, locate the raw files to be analyzed in your file organization structure. Prepare a folder for extracted raw data to be saved to. This should be separate from your tank folder / raw data storage, and will eventually contain the extracted data sets for each session as MATLAB structures to facilitate efficient data processing in future analysis sessions. Subject Key The subject key should contain information about each subject that is constant and unchanging, such as SubjectID, sex, date of birth, fiber location, sensor, experimental group, and any other user specificed information. For example: File Key The file key should contain information about each unique session / file to be analyzed. At a minimum, this must include the SubjectID, folder name, raw folder location, and desired location for the raw data to be exported to. REQUIRED VARIABLES: SubjectID: Unique identifier of the subject. This fieldname should match the first field in the subject key, and inputs for each subject have to match the subject key for subject specific data to be properly associated to fiber photometry data. Folder: The name of the folder containing the raw data. RawFolderPath : The path to the location where the raw data folder is saved for each specific session. All file paths should be specified in the file key WITHOUT the computer user specific portion of the path. This facilitates analysis across multiple devices or cloud storage solutions without manual edits to the file key. Ensure the specified path ends in a forward slash. For example , a file saved to a specific device at the path \"C:\\Users\\rmdon\\Box\\RawData\\\" should be specified as \"Box\\RawData\\\" . ExtractedFolderPath: The path to the location where you would like the extracted data structure to be saved. This should be different than the raw data storage location. All file paths should be specified in the file key WITHOUT the computer and user specific portion of the path. This facilitates analysis across multiple devices or cloud storage solutions without manual edits to the file key. Ensure the specified path ends in a forward slash. For example , a file saved to a specific device at the path \"C:\\Users\\rmdon\\Box\\ExtractedData\\\" should be specified as \"Box\\ExtractedData\\\" . Any additional fields can be included such as equipment information, recording power, session condition, drug treatments, body weight, and any other variables that are specific to that one session. Note that the only field name that should overlap with a field name in the subject key is SubjectID . For example: LoadKeys The function loadKeys joins the individual subject information to the file key with the data for each session. Additionally, loadkeys appends the unique computer user portion of the file navigation path to the beginning and the Folder name to the end of the raw and extracted folder paths specified in the file key. This creates the full path to the location of each session's data. For example, \"C:\\Users\\rmdon\\Box\\RawData\\Subject1-240101-121500\" . The created experiment key should be output into a data structure called experimentkey . REQUIRED INPUTS: computeruserpath: A string containing the portion of the filepath that is unique to the specific computer being used for analysis. This input allows users to easily switch between computers without updating the individual paths in the file key. subjectkeyname: A string containing the name of the csv file that contains the subject key file name, including the .csv extention at the end. If no subject is needed (such as if every subject only has one session of photometry data), then subjectkeyname can be left empty (set to ''). filekeyname: A string containing the name of the csv file that contains the file key file name, including the .csv extention at the end. Code example: Extracting the Data Prior to beginning analysis, individual session data should be extracted and saved as MATLAB data structures. This makes the process of loading data at the start of each analysis session significantly faster. When the raw data is extracted, clipping will be applied by default. This removes the first and last 5 seconds of the session to remove large fluctuations in output signal that occur when the hardware is turned on and off. The number of seconds clipped can be adjusted by overriding the default. extractdata Multiple options are available to extract the data, and should be used depending on the method by which the data was collected, both documented in detail below. Several customized functions are available to extract and format the data for easy processing and analysis. Data collected with TDT can be extracted with custom functions. For all other systems, utilize the generic csv format. The required inputs are the same for all extract data functions. REQUIRED INPUTS: rawfolderpaths: A string array containing the full paths to the folder locations of raw data to be extracted for each session. This should be formatted as a single column with each full path in a separate row. This is easy to create from the experiment key (see below for an example). extractedfolderpaths: A string array containing the full paths to the folder locations where extracted data should be saved for each session (including the individual session name). As with the rawfolderpaths, this should be formatted as a single column with each full path in a separate row and can easily be created from the experiment key (see below for an example). sigstreamnames: A cell array containing the strings with the names of all streams to be treated as signal. This allows for flexibility if different photometry rigs have differing naming conventions for the signal stream. Include all signal stream name variations in this cell array. Note that only one stream per file can be treated as signal. baqstreamnames: A cell array containing the strings with the names of all streams to be treated as background. This allows for flexibility if different photometry rigs have differing naming conventions for the background stream. Include all background stream name variations in this cell array. Note that only one stream per file can be treated as signal. sigstreamnames: A cell array containing the strings with the names of all streams to be treated as signal. This allows for flexibility if different photometry rigs have differing naming conventions for signal stream. Include all signal stream name variations in this cell array. Note that only one stream per file can be treated as signal. Code example: OPTIONAL INPUTS: clip: Number of seconds to clip at the beginning and end of the session. This defaults to 5 seconds. skipexisting: This input allows users to toggle if previously extracted raw data files are re-extracted. By default, previously extracted files will be skipped (skipexisting = 1). To override and re-extract all files, set skipexisting = 0. Extracted raw data files are saved to the extracted folder path as individual MATLAB structures. Loading the Data After raw data is extracted, it can be matched to the experimentkey to associate any subject and session metadata with the photometry data. All sessions will be loaded into one data structure (typically called rawdata ), to ensure that all sessions are analyzed in the same way throughout following steps of the protocol. Each session of data is a row within the data structure. Regardless of hardware set up, all extracted files can be loaded with the function LoadKeydata . loadKeydata REQUIRED INPUTS: experimentkey: Data structure created by the LoadKeys function, containing at least the field ExtractedFolderPath with the full path to each individual session of data to be loaded. Code example: Trimming the Data The final step in data preparation is optional. Photometry recording may start a few seconds before the experiment begins, such as in cases where users have to initiate hardware for operant boxes separately, and after the experiment ends. Additionally, users may want to remove the first few minutes of each session due to the higher rate of photobleaching before the signal stabilizes. If desired, data can be trimmed with the function trimFPdata , which uses user derived session start and session end indexes to trim data streams and adjust any event epochs (timestamps) to maintain the relationship in time. Index Preparation If timestamps for session start and end are included in the raw data collection, then these fields can be used as it. If not, users must first determine the appropriate start and end points from whatever timestamps are relevant. For example, in the example analysis provided the session is trimmed to 15 minutes before the first injection time stamp, and 60 minutes after the second injection time stamp. Code example of preparing the start and end indices: trimFPdata REQUIRED INPUTS: data: Data structure created by the LoadKeyData function. Each session should be a separate row. The data structure must containing at least the fields specified in the additional inputs. whichtrimstart: A string containing the name of the field with the locations of the session start indices. Everything before the start index will be trimmed. whichtrimend: A string containing the name of the field with the locations of the session end indices. Everything after the end index will be trimmed. whichstreams: A cell array containing the field names of all the streams to be trimmed. This should include both the signal and the background streams. OPTIONAL INPUTS: whichepocs: A cell array containing the field names of all the epochs (time stamps) to be adjusted. This input can contain as many inputs as the experimental paradigm requires, and each timestamp will be adjusted by subtracting the start index - 1. Code example of trimming: After data preparation is complete, move to signal processing.","title":"Data Preparation"},{"location":"userguide/datapreparation/#data-preparation","text":"The first stage in the PASTa Protocol is data preparation, which involves associating fiber photometry data with experimental meta data, loading raw data, and extracting and saving the raw data as MATLAB data structures to faciliate future analysis sessions. For data collected with Tucker Davis Technologies equipment and software Synapse , custom functions are included to extract data from raw formats. For data collected with other systems, data must be pre-formatted to match a generic csv file format structure and loaded with provided custom functions, or prepared separately by the user. If your data doesn't match the available load options, please feel free to reach out!","title":"Data Preparation"},{"location":"userguide/datapreparation/#data-organization","text":"The PASTa protocol is set up to accomodate any kind of file organization preferred by the user. Organization preference may vary depending on your lab's file storage practices, photometry equipment, or individual projects.","title":"Data Organization"},{"location":"userguide/datapreparation/#tdt-synapse-output","text":"Fiber photometry data collected through the software Synapse (Tucker Davis Technologies) is stored in tanks and blocks . Tanks are parent folders created by Synapse for each experiment. Blocks are individual folders for each session containing the actual output data. Stored data cannot be accessed directly via the folder, but rather must be extracted via MATLAB. By default, the tank path is: C:\\TDT\\Synapse\\Tanks. Synapse recognizes experiments and subjects as key categories of information that play a special role in managing data storage and retrieval. Thus, when running the session, it is critical to ensure the correct Experiment profile is selected, and the correct Subject identifier is input for each session. By default, Synapse names data tanks automatically based on experiment name and the start time of the first recording {ExperimentName}-{yymmdd}-{hhmmss}. Blocks of data are named based on subject {SubjectName}-{yymmdd}-{hhmmss} for each recording session and the start time. Synapse will save a new Tank for every day unless you change the default setting. Click Menu at the top of the bar, then Preferences. Under the Data Saving tab, make sure \"New Tank Each Day\" is unchecked.","title":"TDT Synapse Output"},{"location":"userguide/datapreparation/#experiment-key-creation","text":"To accomodate a variety of file organization structures, users can first create two csv files containing the information necessary to access raw data, and experimental metadata to match to raw photometry data. MATLAB can access raw data folders stored either locally or in a cloud-based storage app like Box or Dropbox. To faciliate analysis, users can create two keys as csv files: a subject key and a file key. In the PASTa protocol, these files will be knit together to pair subject specific information with each individual session of data, preventing the need for manual repeated entry of subject specific information and reduce the time burden of properly maintaining and including experimental metadata factors like subject traits, treatments, and experimental equipment. First, locate the raw files to be analyzed in your file organization structure. Prepare a folder for extracted raw data to be saved to. This should be separate from your tank folder / raw data storage, and will eventually contain the extracted data sets for each session as MATLAB structures to facilitate efficient data processing in future analysis sessions.","title":"Experiment Key Creation"},{"location":"userguide/datapreparation/#subject-key","text":"The subject key should contain information about each subject that is constant and unchanging, such as SubjectID, sex, date of birth, fiber location, sensor, experimental group, and any other user specificed information. For example:","title":"Subject Key"},{"location":"userguide/datapreparation/#file-key","text":"The file key should contain information about each unique session / file to be analyzed. At a minimum, this must include the SubjectID, folder name, raw folder location, and desired location for the raw data to be exported to. REQUIRED VARIABLES: SubjectID: Unique identifier of the subject. This fieldname should match the first field in the subject key, and inputs for each subject have to match the subject key for subject specific data to be properly associated to fiber photometry data. Folder: The name of the folder containing the raw data. RawFolderPath : The path to the location where the raw data folder is saved for each specific session. All file paths should be specified in the file key WITHOUT the computer user specific portion of the path. This facilitates analysis across multiple devices or cloud storage solutions without manual edits to the file key. Ensure the specified path ends in a forward slash. For example , a file saved to a specific device at the path \"C:\\Users\\rmdon\\Box\\RawData\\\" should be specified as \"Box\\RawData\\\" . ExtractedFolderPath: The path to the location where you would like the extracted data structure to be saved. This should be different than the raw data storage location. All file paths should be specified in the file key WITHOUT the computer and user specific portion of the path. This facilitates analysis across multiple devices or cloud storage solutions without manual edits to the file key. Ensure the specified path ends in a forward slash. For example , a file saved to a specific device at the path \"C:\\Users\\rmdon\\Box\\ExtractedData\\\" should be specified as \"Box\\ExtractedData\\\" . Any additional fields can be included such as equipment information, recording power, session condition, drug treatments, body weight, and any other variables that are specific to that one session. Note that the only field name that should overlap with a field name in the subject key is SubjectID . For example:","title":"File Key"},{"location":"userguide/datapreparation/#loadkeys","text":"The function loadKeys joins the individual subject information to the file key with the data for each session. Additionally, loadkeys appends the unique computer user portion of the file navigation path to the beginning and the Folder name to the end of the raw and extracted folder paths specified in the file key. This creates the full path to the location of each session's data. For example, \"C:\\Users\\rmdon\\Box\\RawData\\Subject1-240101-121500\" . The created experiment key should be output into a data structure called experimentkey . REQUIRED INPUTS: computeruserpath: A string containing the portion of the filepath that is unique to the specific computer being used for analysis. This input allows users to easily switch between computers without updating the individual paths in the file key. subjectkeyname: A string containing the name of the csv file that contains the subject key file name, including the .csv extention at the end. If no subject is needed (such as if every subject only has one session of photometry data), then subjectkeyname can be left empty (set to ''). filekeyname: A string containing the name of the csv file that contains the file key file name, including the .csv extention at the end. Code example:","title":"LoadKeys"},{"location":"userguide/datapreparation/#extracting-the-data","text":"Prior to beginning analysis, individual session data should be extracted and saved as MATLAB data structures. This makes the process of loading data at the start of each analysis session significantly faster. When the raw data is extracted, clipping will be applied by default. This removes the first and last 5 seconds of the session to remove large fluctuations in output signal that occur when the hardware is turned on and off. The number of seconds clipped can be adjusted by overriding the default.","title":"Extracting the Data"},{"location":"userguide/datapreparation/#extractdata","text":"Multiple options are available to extract the data, and should be used depending on the method by which the data was collected, both documented in detail below. Several customized functions are available to extract and format the data for easy processing and analysis. Data collected with TDT can be extracted with custom functions. For all other systems, utilize the generic csv format. The required inputs are the same for all extract data functions. REQUIRED INPUTS: rawfolderpaths: A string array containing the full paths to the folder locations of raw data to be extracted for each session. This should be formatted as a single column with each full path in a separate row. This is easy to create from the experiment key (see below for an example). extractedfolderpaths: A string array containing the full paths to the folder locations where extracted data should be saved for each session (including the individual session name). As with the rawfolderpaths, this should be formatted as a single column with each full path in a separate row and can easily be created from the experiment key (see below for an example). sigstreamnames: A cell array containing the strings with the names of all streams to be treated as signal. This allows for flexibility if different photometry rigs have differing naming conventions for the signal stream. Include all signal stream name variations in this cell array. Note that only one stream per file can be treated as signal. baqstreamnames: A cell array containing the strings with the names of all streams to be treated as background. This allows for flexibility if different photometry rigs have differing naming conventions for the background stream. Include all background stream name variations in this cell array. Note that only one stream per file can be treated as signal. sigstreamnames: A cell array containing the strings with the names of all streams to be treated as signal. This allows for flexibility if different photometry rigs have differing naming conventions for signal stream. Include all signal stream name variations in this cell array. Note that only one stream per file can be treated as signal. Code example: OPTIONAL INPUTS: clip: Number of seconds to clip at the beginning and end of the session. This defaults to 5 seconds. skipexisting: This input allows users to toggle if previously extracted raw data files are re-extracted. By default, previously extracted files will be skipped (skipexisting = 1). To override and re-extract all files, set skipexisting = 0. Extracted raw data files are saved to the extracted folder path as individual MATLAB structures.","title":"extractdata"},{"location":"userguide/datapreparation/#loading-the-data","text":"After raw data is extracted, it can be matched to the experimentkey to associate any subject and session metadata with the photometry data. All sessions will be loaded into one data structure (typically called rawdata ), to ensure that all sessions are analyzed in the same way throughout following steps of the protocol. Each session of data is a row within the data structure. Regardless of hardware set up, all extracted files can be loaded with the function LoadKeydata .","title":"Loading the Data"},{"location":"userguide/datapreparation/#loadkeydata","text":"REQUIRED INPUTS: experimentkey: Data structure created by the LoadKeys function, containing at least the field ExtractedFolderPath with the full path to each individual session of data to be loaded. Code example:","title":"loadKeydata"},{"location":"userguide/datapreparation/#trimming-the-data","text":"The final step in data preparation is optional. Photometry recording may start a few seconds before the experiment begins, such as in cases where users have to initiate hardware for operant boxes separately, and after the experiment ends. Additionally, users may want to remove the first few minutes of each session due to the higher rate of photobleaching before the signal stabilizes. If desired, data can be trimmed with the function trimFPdata , which uses user derived session start and session end indexes to trim data streams and adjust any event epochs (timestamps) to maintain the relationship in time.","title":"Trimming the Data"},{"location":"userguide/datapreparation/#index-preparation","text":"If timestamps for session start and end are included in the raw data collection, then these fields can be used as it. If not, users must first determine the appropriate start and end points from whatever timestamps are relevant. For example, in the example analysis provided the session is trimmed to 15 minutes before the first injection time stamp, and 60 minutes after the second injection time stamp. Code example of preparing the start and end indices:","title":"Index Preparation"},{"location":"userguide/datapreparation/#trimfpdata","text":"REQUIRED INPUTS: data: Data structure created by the LoadKeyData function. Each session should be a separate row. The data structure must containing at least the fields specified in the additional inputs. whichtrimstart: A string containing the name of the field with the locations of the session start indices. Everything before the start index will be trimmed. whichtrimend: A string containing the name of the field with the locations of the session end indices. Everything after the end index will be trimmed. whichstreams: A cell array containing the field names of all the streams to be trimmed. This should include both the signal and the background streams. OPTIONAL INPUTS: whichepocs: A cell array containing the field names of all the epochs (time stamps) to be adjusted. This input can contain as many inputs as the experimental paradigm requires, and each timestamp will be adjusted by subtracting the start index - 1. Code example of trimming: After data preparation is complete, move to signal processing.","title":"trimFPdata"},{"location":"userguide/signalprocessing/","text":"Signal Processing After raw photometry data is loaded in to MATLAB, signal processing is conducted to account for photobleaching, motion artifacts, and other sources of \"noise\". The signal processing functions are written to be as flexible to differing streams and naming conventions as possible, but if the functions don't match your data, please reach out and let us know and we will update. Background Scaling and Subtraction To correct for photobleaching and motion artifact, the background is scaled to the signal stream and subtracted. While previous tools and packages have used regression based approaches, PASTa scales the background based on the frequency domain of the streams, preventing over or underfitting and maintaining the shape of the background. Background Scaling Factor To determine the scaling factor for the background stream relative to the signal stream, both the signal and background streams are converted to the frequency domain via Fast Fourier Transform (FFT). The scaling factor is calculated as for all frequencies greater than 10 Hz. The background stream is scaled in the time domain; the background is centered around zero, multiplied by the scaling factor, and then adjusted to center around the signal mean. Subtraction The scaled background is subtracted from the raw signal. Subtracted signal is output as dF/F. Users have the ability to override PASTa protocol defaults to modify parameters including the scaling threshold frequency cutoff, subtracted data output, or select an alternative method of background scaling. Other Scaling Methods In our hands, frequency scaling performs better than other scaling approaches. Advantages are particularly notable for sensors such as GRABDA2H, for which 405nm is not a perfect isosbestic control. Use of frequency scaling rescues the use of the 405nm stream to control for photobleaching and motion artifact. This may be particularly useful as new sensors are continually in development, not all of which have an isosbestic or commercially available control wavelength for use in photometry systems. However, users may want to compare to other commonly used methods of scaling the background to the signal. The subtractFPdata function includes options to use OLS regression, detrending and OLS regression, Lowess Smoothing and OLS Regression, and IRLS regression to scale the background prior to subtraction. Filtering After subtraction, the signal is filtered to remove high frequency noise and facilitate further analysis. PASTa protocol has default filter settings, but users can override these as required by sensor or experimental design. Filter type: defaults to a 3rd order Butterworth band-pass filter, which preserves the shape of the signal in the pass band. Filter cutoffs: The high-pass cutoff is set to 0.0051 Hz to remove the zero frequency component of the power spectrum. The low-pass cutoff is set to 2.2860 Hz to preserve the frequencies of interest while attenuating high frequency noise. Users can override defaults to modify the filter type (band-pass, high-pass, low-pass), order, and cutoff frequencies. Normalization Normalization converts the filtered signal to Z score. Multiple methods are included in the tool box to accomodate a variety of experimental designs. Whole session uses the mean and SD of the whole session. Session baseline uses the mean and SD from a specified session baseline, which may be useful in cases where a drug is delivered mid session. Data can also be normalized on an individual trial basis to a local pre-trial baseline. Differing normalization methods may be advantageous depending on the experimental design. If additional options are required, please let us know!","title":"Signal Processing"},{"location":"userguide/signalprocessing/#signal-processing","text":"After raw photometry data is loaded in to MATLAB, signal processing is conducted to account for photobleaching, motion artifacts, and other sources of \"noise\". The signal processing functions are written to be as flexible to differing streams and naming conventions as possible, but if the functions don't match your data, please reach out and let us know and we will update.","title":"Signal Processing"},{"location":"userguide/signalprocessing/#background-scaling-and-subtraction","text":"To correct for photobleaching and motion artifact, the background is scaled to the signal stream and subtracted. While previous tools and packages have used regression based approaches, PASTa scales the background based on the frequency domain of the streams, preventing over or underfitting and maintaining the shape of the background.","title":"Background Scaling and Subtraction"},{"location":"userguide/signalprocessing/#background-scaling-factor","text":"To determine the scaling factor for the background stream relative to the signal stream, both the signal and background streams are converted to the frequency domain via Fast Fourier Transform (FFT). The scaling factor is calculated as for all frequencies greater than 10 Hz. The background stream is scaled in the time domain; the background is centered around zero, multiplied by the scaling factor, and then adjusted to center around the signal mean.","title":"Background Scaling Factor"},{"location":"userguide/signalprocessing/#subtraction","text":"The scaled background is subtracted from the raw signal. Subtracted signal is output as dF/F. Users have the ability to override PASTa protocol defaults to modify parameters including the scaling threshold frequency cutoff, subtracted data output, or select an alternative method of background scaling.","title":"Subtraction"},{"location":"userguide/signalprocessing/#other-scaling-methods","text":"In our hands, frequency scaling performs better than other scaling approaches. Advantages are particularly notable for sensors such as GRABDA2H, for which 405nm is not a perfect isosbestic control. Use of frequency scaling rescues the use of the 405nm stream to control for photobleaching and motion artifact. This may be particularly useful as new sensors are continually in development, not all of which have an isosbestic or commercially available control wavelength for use in photometry systems. However, users may want to compare to other commonly used methods of scaling the background to the signal. The subtractFPdata function includes options to use OLS regression, detrending and OLS regression, Lowess Smoothing and OLS Regression, and IRLS regression to scale the background prior to subtraction.","title":"Other Scaling Methods"},{"location":"userguide/signalprocessing/#filtering","text":"After subtraction, the signal is filtered to remove high frequency noise and facilitate further analysis. PASTa protocol has default filter settings, but users can override these as required by sensor or experimental design. Filter type: defaults to a 3rd order Butterworth band-pass filter, which preserves the shape of the signal in the pass band. Filter cutoffs: The high-pass cutoff is set to 0.0051 Hz to remove the zero frequency component of the power spectrum. The low-pass cutoff is set to 2.2860 Hz to preserve the frequencies of interest while attenuating high frequency noise. Users can override defaults to modify the filter type (band-pass, high-pass, low-pass), order, and cutoff frequencies.","title":"Filtering"},{"location":"userguide/signalprocessing/#normalization","text":"Normalization converts the filtered signal to Z score. Multiple methods are included in the tool box to accomodate a variety of experimental designs. Whole session uses the mean and SD of the whole session. Session baseline uses the mean and SD from a specified session baseline, which may be useful in cases where a drug is delivered mid session. Data can also be normalized on an individual trial basis to a local pre-trial baseline. Differing normalization methods may be advantageous depending on the experimental design. If additional options are required, please let us know!","title":"Normalization"},{"location":"userguide/transientdetection/","text":"Transient Detection and Quantification Transient detection is a critical component of fiber photometry analyses, identifying relevant increases in sensor activation. Previous tools and packages have used a sliding window approach, where all values above an absolute threshold are counted as peaks. Here, we present a novel method of peak detection where each peak is compared to a local baseline and amplitude is calculated and compared to a threshold to determine inclusion. This allows for consistent parameters across the session and reliable detection of individual events despite signal absolute value fluctuation. Transient Event Detection Transients are detected as peaks with a greater amplitude than the specified threshold. To determine amplitude, first a pre-peak baseline must be identified. PASTa includes three options: window minimum (minimum value within user defined pre-peak window size, e.g. 800ms), window mean (mean of user defined pre-peak window), or local min (absolute local minimum preceding the peak). The amplitude threshold is set by the user, and recommended to be 3SDs. If data are normalized in Z scores, then the criterion is an increase of 3 from baseline. If not, the user inputs the actual value that corresponds to 3SDs in the data stream. Transient Event Quantification Multiple features of transient events can be quantitatively analyzed and compared. Peak detection functions automatically calculate numerous variables for each transient to characterize aspects of both event rise and fall. PASTa includes flexible functions to group transients in the most experimentally relevant manner, such as by time window or experimental condition. Frequency : Characterized as peaks per minute. Frequency can be analyzed as whole session frequency, or peaks can be divided into time bins or experimental phases (ITI, during trial, etc). Amplitude : The height of the event from the pre-peak baseline to the max peak. Note that all events will be at least the value of the set threshold (default 3SD). Rise and Fall Time : The time difference between the pre-peak baseline and the max peak value. Rise and fall time can also be characterized from half height to peak, allowing analysis of separate rise and fall dynamic shifts. AUC : Total area under the curve from half height to peak, calculated via the trapezoidal method. Prior to AUC calculation, each transient is linearly transformed so pre-peak baseline is equal to zero.","title":"Transient Detection"},{"location":"userguide/transientdetection/#transient-detection-and-quantification","text":"Transient detection is a critical component of fiber photometry analyses, identifying relevant increases in sensor activation. Previous tools and packages have used a sliding window approach, where all values above an absolute threshold are counted as peaks. Here, we present a novel method of peak detection where each peak is compared to a local baseline and amplitude is calculated and compared to a threshold to determine inclusion. This allows for consistent parameters across the session and reliable detection of individual events despite signal absolute value fluctuation.","title":"Transient Detection and Quantification"},{"location":"userguide/transientdetection/#transient-event-detection","text":"Transients are detected as peaks with a greater amplitude than the specified threshold. To determine amplitude, first a pre-peak baseline must be identified. PASTa includes three options: window minimum (minimum value within user defined pre-peak window size, e.g. 800ms), window mean (mean of user defined pre-peak window), or local min (absolute local minimum preceding the peak). The amplitude threshold is set by the user, and recommended to be 3SDs. If data are normalized in Z scores, then the criterion is an increase of 3 from baseline. If not, the user inputs the actual value that corresponds to 3SDs in the data stream.","title":"Transient Event Detection"},{"location":"userguide/transientdetection/#transient-event-quantification","text":"Multiple features of transient events can be quantitatively analyzed and compared. Peak detection functions automatically calculate numerous variables for each transient to characterize aspects of both event rise and fall. PASTa includes flexible functions to group transients in the most experimentally relevant manner, such as by time window or experimental condition. Frequency : Characterized as peaks per minute. Frequency can be analyzed as whole session frequency, or peaks can be divided into time bins or experimental phases (ITI, during trial, etc). Amplitude : The height of the event from the pre-peak baseline to the max peak. Note that all events will be at least the value of the set threshold (default 3SD). Rise and Fall Time : The time difference between the pre-peak baseline and the max peak value. Rise and fall time can also be characterized from half height to peak, allowing analysis of separate rise and fall dynamic shifts. AUC : Total area under the curve from half height to peak, calculated via the trapezoidal method. Prior to AUC calculation, each transient is linearly transformed so pre-peak baseline is equal to zero.","title":"Transient Event Quantification"},{"location":"userguide/userguide/","text":"PASTa User Guide This user guide is meant to serve as a detailed step-by-step practical guide through preparing, processing, and analyzing fiber photometry data with the PASTa protocol.","title":"User Guide"},{"location":"userguide/userguide/#pasta-user-guide","text":"This user guide is meant to serve as a detailed step-by-step practical guide through preparing, processing, and analyzing fiber photometry data with the PASTa protocol.","title":"PASTa User Guide"}]}